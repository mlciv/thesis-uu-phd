%%% -*-LaTeX-*-
%%% This is the abstract for the thesis.
%%% It is included in the top-level LaTeX file with
%%%
%%%    \preface    {abstract} {Abstract}
%%%
%%% The first argument is the basename of this file, and the
%%% second is the title for this page, which is thus not
%%% included here.
%%%
%%% The text of this file should be about 350 words or less.
  Natural language, when described as a system of symbolic human
communication, it is inherently studied via various discrete symbolic
representations. Designing the artificial symbolic representation for
natural language intuitively follows the principle of compositionality
in natural language. Leveraging this as an inductive bias, we conduct
systematic studies on factorization-oriented representation learning
to predict those combinatorial and symbolic structures. We propose the
corresponding neural representation learning approaches for three
different kinds of factorization: {\it Independent Factorization},
{\it Label Representation}, {\it Auto-regressive Factorization}. We
ground our studies on both broad-coverage linguistic
representations~(such as graph-based meaning representations, DM, PSD,
AMR, and UCCA, etc.)~and application-specific representations~(such as
MISC code for psychotherapy dialog and dialog state representations
for task-oriented dialog). The experimental results show our proposed
methods achieve competetive performance on each task, and they
potentially help for other linguistic structured prediction tasks with
similar factorizations.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis-main.ltx"
%%% End:
