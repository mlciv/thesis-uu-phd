@article{tsochantaridis2005large,
  title={Large margin methods for structured and interdependent output variables.},
  author={Tsochantaridis, Ioannis and Joachims, Thorsten and Hofmann, Thomas and Altun, Yasemin and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={6},
  number={9},
  year={2005}
}

@article{koller2004max,
  title={Max-margin Markov networks},
  author={Koller, Ben Taskar Carlos Guestrin Daphne},
  journal={Advances in Neural Information Processing Systems},
  volume={16},
  pages={25},
  year={2004}
}

@article{tu2018learning,
  title={Learning Approximate Inference Networks for Structured Prediction},
  author={Tu, Lifu and Gimpel, Kevin},
  journal={International Conference on Learning Representations},
  year={2018}
}

@article{smith2011linguistic,
  title={Linguistic structure prediction},
  author={Smith, Noah A},
  journal={Synthesis Lectures on Human Language Technologies},
  volume={4},
  number={2},
  pages={1--274},
  year={2011},
  publisher={Morgan \& Claypool Publishers}
}

@inproceedings{moosavi-strube-2018-using,
    title = "Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers",
    author = "Moosavi, Nafise Sadat  and
      Strube, Michael",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1018",
    doi = "10.18653/v1/D18-1018",
    pages = "193--203",
    abstract = "Coreference resolution is an intermediate step for text understanding. It is used in tasks and domains for which we do not necessarily have coreference annotated corpora. Therefore, generalization is of special importance for coreference resolution. However, while recent coreference resolvers have notable improvements on the CoNLL dataset, they struggle to generalize properly to new domains or datasets. In this paper, we investigate the role of linguistic features in building more generalizable coreference resolvers. We show that generalization improves only slightly by merely using a set of additional linguistic features. However, employing features and subsets of their values that are informative for coreference resolution, considerably improves generalization. Thanks to better generalization, our system achieves state-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref, our system, which is trained on CoNLL, achieves on-par performance with a system designed for this dataset.",
}

@inproceedings{strubell-etal-2018-linguistically,
    title = "Linguistically-Informed Self-Attention for Semantic Role Labeling",
    author = "Strubell, Emma  and
      Verga, Patrick  and
      Andor, Daniel  and
      Weiss, David  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1548",
    doi = "10.18653/v1/D18-1548",
    pages = "5027--5038",
    abstract = "Current state-of-the-art semantic role labeling (SRL) uses a deep neural network with no explicit linguistic features. However, prior work has shown that gold syntax trees can dramatically improve SRL decoding, suggesting the possibility of increased accuracy from explicit modeling of syntax. In this work, we present linguistically-informed self-attention (LISA): a neural network model that combines multi-head self-attention with multi-task learning across dependency parsing, part-of-speech tagging, predicate detection and SRL. Unlike previous models which require significant pre-processing to prepare linguistic features, LISA can incorporate syntax using merely raw tokens as input, encoding the sequence only once to simultaneously perform parsing, predicate detection and role labeling for all predicates. Syntax is incorporated by training one attention head to attend to syntactic parents for each token. Moreover, if a high-quality syntactic parse is already available, it can be beneficially injected at test time without re-training our SRL model. In experiments on CoNLL-2005 SRL, LISA achieves new state-of-the-art performance for a model using predicted predicates and standard word embeddings, attaining 2.5 F1 absolute higher than the previous state-of-the-art on newswire and more than 3.5 F1 on out-of-domain data, nearly 10{\%} reduction in error. On ConLL-2012 English SRL we also show an improvement of more than 2.5 F1. LISA also out-performs the state-of-the-art with contextually-encoded (ELMo) word representations, by nearly 1.0 F1 on news and more than 2.0 F1 on out-of-domain text.",
}

@inproceedings{bowman-etal-2016-fast,
    title = "A Fast Unified Model for Parsing and Sentence Understanding",
    author = "Bowman, Samuel R.  and
      Gauthier, Jon  and
      Rastogi, Abhinav  and
      Gupta, Raghav  and
      Manning, Christopher D.  and
      Potts, Christopher",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1139",
    doi = "10.18653/v1/P16-1139",
    pages = "1466--1477",
}


@inproceedings{hovy2010s,
  title={What’s in a preposition? Dimensions of sense disambiguation for an interesting word class},
  author={Hovy, Dirk and Tratz, Stephen and Hovy, Eduard},
  booktitle={Coling 2010: Posters},
  pages={454--462},
  year={2010}
}

@article{punyakanok2008importance,
  title={The importance of syntactic parsing and inference in semantic role labeling},
  author={Punyakanok, Vasin and Roth, Dan and Yih, Wen-tau},
  journal={Computational Linguistics},
  volume={34},
  number={2},
  pages={257--287},
  year={2008},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{xue2004calibrating,
  title={Calibrating features for semantic role labeling.},
  author={Xue, Nianwen and Palmer, Martha},
  booktitle={EMNLP},
  pages={88--94},
  year={2004}
}

@article{werner2014power,
  title={The power of lp relaxation for map inference},
  author={Werner, Tom{\'a}ˇs and Pruˇsa, Daniel},
  journal={Advanced Structured Prediction},
  pages={19},
  year={2014},
  publisher={MIT Press}
}

@inproceedings{finkel2005incorporating,
    title = "Incorporating Non-local Information into Information Extraction Systems by {G}ibbs Sampling",
    author = "Finkel, Jenny Rose  and
      Grenager, Trond  and
      Manning, Christopher",
    booktitle = "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05)",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P05-1045",
    doi = "10.3115/1219840.1219885",
    pages = "363--370",
}

@inproceedings{singh2012monte,
  title={Monte Carlo MCMC: efficient inference by approximate sampling},
  author={Singh, Sameer and Wick, Michael and McCallum, Andrew},
  booktitle={Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
  pages={1104--1113},
  year={2012}
}

@inproceedings{roth2005integer,
  title={Integer linear programming inference for conditional random fields},
  author={Roth, Dan and Yih, Wen-tau},
  booktitle={Proceedings of the 22nd International Conference on Machine Learning},
  pages={736--743},
  year={2005}
}

@inproceedings{berant2014modeling,
  title={Modeling biological processes for reading comprehension},
  author={Berant, Jonathan and Srikumar, Vivek and Chen, Pei-Chun and Vander Linden, Abby and Harding, Brittany and Huang, Brad and Clark, Peter and Manning, Christopher D},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1499--1510},
  year={2014}
}

@article{daume2009search,
  title={Search-based structured prediction},
  author={Daum{\'e}, Hal and Langford, John and Marcu, Daniel},
  journal={Machine Learning},
  volume={75},
  number={3},
  pages={297--325},
  year={2009},
  publisher={Springer}
}

@inproceedings{chang2015learning,
  title={Learning to search better than your teacher},
  author={Chang, Kai-Wei and Krishnamurthy, Akshay and Agarwal, Alekh and Daum{\'e} III, Hal and Langford, John},
  booktitle={International Conference on Machine Learning},
  pages={2058--2066},
  year={2015},
  organization={PMLR}
}

@article{rush2012tutorial,
  title={A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing},
  author={Rush, Alexander M and Collins, MJ},
  journal={Journal of Artificial Intelligence Research},
  volume={45},
  pages={305--362},
  year={2012}
}

@book{cocke1969programming,
  title={Programming Languages and Their Compilers: Preliminary Notes},
  author={Cocke, John},
  year={1969},
  publisher={New York University}
}

@article{liu2018learning,
  title={Learning structured text representations},
  author={Liu, Yang and Lapata, Mirella},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={63--75},
  year={2018},
  publisher={MIT Press}
}

@inproceedings{koo-etal-2007-structured,
    title = "Structured Prediction Models via the Matrix-Tree Theorem",
    author = "Koo, Terry  and
      Globerson, Amir  and
      Carreras, Xavier  and
      Collins, Michael",
    booktitle = "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D07-1015",
    pages = "141--150",
}

@inproceedings{smith2005contrastive,
    title = "Contrastive Estimation: Training Log-Linear Models on Unlabeled Data",
    author = "Smith, Noah A.  and
      Eisner, Jason",
    booktitle = "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05)",
    month = jun,
    year = 2005,
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P05-1044",
    doi = "10.3115/1219840.1219884",
    pages = "354--362",
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={627--635},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{toutanova2003feature,
  title={Feature-rich part-of-speech tagging with a cyclic dependency network},
  author={Toutanova, Kristina and Klein, Dan and Manning, Christopher D and Singer, Yoram},
  booktitle={Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={252--259},
  year={2003}
}

@inproceedings{toutanvoa2000enriching,
  title={Enriching the knowledge sources used in a maximum entropy part-of-speech tagger},
  author={Toutanvoa, Kristina and Manning, Christopher D},
  booktitle={2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora},
  pages={63--70},
  year={2000}
}

@article{liu2021pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={arXiv preprint arXiv:2107.13586},
  year={2021}
}

@inproceedings{ethayarajh2019contextual,
    title = "How Contextual are Contextualized Word Representations? {C}omparing the Geometry of {BERT}, {ELM}o, and {GPT}-2 Embeddings",
    author = "Ethayarajh, Kawin",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = 2019,
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1006",
    doi = "10.18653/v1/D19-1006",
    pages = "55--65",
    abstract = "Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5{\%} of the variance in a word{'}s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.",
}

@inproceedings{shin2020autoprompt,
    title = "{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts",
    author = "Shin, Taylor  and
      Razeghi, Yasaman  and
      Logan IV, Robert L.  and
      Wallace, Eric  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.346",
    doi = "10.18653/v1/2020.emnlp-main.346",
    pages = "4222--4235",
    abstract = "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.",
}

@inproceedings{lazaridou-etal-2015-combining,
    title = "Combining Language and Vision with a Multimodal Skip-gram Model",
    author = "Lazaridou, Angeliki  and
      Pham, Nghia The  and
      Baroni, Marco",
    booktitle = "Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = may # "{--}" # jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N15-1016",
    doi = "10.3115/v1/N15-1016",
    pages = "153--163",
}

@inproceedings{beinborn-etal-2018-multimodal,
    title = "Multimodal Grounding for Language Processing",
    author = "Beinborn, Lisa  and
      Botschen, Teresa  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1197",
    pages = "2325--2339",
    abstract = "This survey discusses how recent developments in multimodal processing facilitate conceptual grounding of language. We categorize the information flow in multimodal processing with respect to cognitive models of human information processing and analyze different methods for combining multimodal representations. Based on this methodological inventory, we discuss the benefit of multimodal grounding for a variety of language processing tasks and the challenges that arise. We particularly focus on multimodal grounding of verbs which play a crucial role for the compositional power of language.",
}

@inproceedings{ling-etal-2022-vision,
    title = "Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis",
    author = "Ling, Yan  and
      Yu, Jianfei  and
      Xia, Rui",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.152",
    doi = "10.18653/v1/2022.acl-long.152",
    pages = "2149--2159",
    abstract = "As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment Analysis (MABSA) has attracted increasing attention inrecent years. However, previous approaches either (i) use separately pre-trained visual and textual models, which ignore the crossmodalalignment or (ii) use vision-language models pre-trained with general pre-training tasks, which are inadequate to identify fine-grainedaspects, opinions, and their alignments across modalities. To tackle these limitations, we propose a task-specific Vision-LanguagePre-training framework for MABSA (VLP-MABSA), which is a unified multimodal encoder-decoder architecture for all the pretrainingand downstream tasks. We further design three types of task-specific pre-training tasks from the language, vision, and multimodalmodalities, respectively. Experimental results show that our approach generally outperforms the state-of-the-art approaches on three MABSA subtasks. Further analysis demonstrates the effectiveness of each pre-training task. The source code is publicly released at https://github.com/NUSTM/VLP-MABSA.",
}

@inproceedings{ju2021joint,
  title={Joint multi-modal aspect-sentiment analysis with auxiliary cross-modal relation detection},
  author={Ju, Xincheng and Zhang, Dong and Xiao, Rong and Li, Junhui and Li, Shoushan and Zhang, Min and Zhou, Guodong},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={4395--4405},
  year={2021}
}

@inproceedings{zhang2021multi,
  title={Multi-modal graph fusion for named entity recognition with targeted visual guidance},
  author={Zhang, Dong and Wei, Suzhong and Li, Shoushan and Wu, Hanqian and Zhu, Qiaoming and Zhou, Guodong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume=35,
  pages={14347--14355},
  year=2021
}

@inproceedings{singh-etal-2021-mimoqa,
    title = "{MIMOQA}: Multimodal Input Multimodal Output Question Answering",
    author = "Singh, Hrituraj  and
      Nasery, Anshul  and
      Mehta, Denil  and
      Agarwal, Aishwarya  and
      Lamba, Jatin  and
      Srinivasan, Balaji Vasan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.418",
    doi = "10.18653/v1/2021.naacl-main.418",
    pages = "5317--5332",
    abstract = "Multimodal research has picked up significantly in the space of question answering with the task being extended to visual question answering, charts question answering as well as multimodal input question answering. However, all these explorations produce a unimodal textual output as the answer. In this paper, we propose a novel task - MIMOQA - Multimodal Input Multimodal Output Question Answering in which the output is also multimodal. Through human experiments, we empirically show that such multimodal outputs provide better cognitive understanding of the answers. We also propose a novel multimodal question-answering framework, MExBERT, that incorporates a joint textual and visual attention towards producing such a multimodal output. Our method relies on a novel multimodal dataset curated for this problem from publicly available unimodal datasets. We show the superior performance of MExBERT against strong baselines on both the automatic as well as human metrics.",
}

@article{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{jones1972statistical,
    author = {Karen Spärck Jones},
    title = {A statistical interpretation of term specificity and its application in retrieval},
    journal = {Journal of Documentation},
    year = {1972},
    volume = {28},
    pages = {11--21}
}

@article{wang2020survey,
  title={A survey on Bayesian deep learning},
  author={Wang, Hao and Yeung, Dit-Yan},
  journal={ACM Computing Surveys (CSUR)},
  volume={53},
  number={5},
  pages={1--37},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{huang2022sdq,
  title={SDQ: Stochastic Differentiable Quantization with Mixed Precision},
  author={Huang, Xijie and Shen, Zhiqiang and Li, Shichao and Liu, Zechun and Xianghong, Hu and Wicaksana, Jeffry and Xing, Eric and Cheng, Kwang-Ting},
  booktitle={International Conference on Machine Learning},
  pages={9295--9309},
  year={2022},
  organization={PMLR}
}

@inproceedings{assran2019stochastic,
  title={Stochastic gradient push for distributed deep learning},
  author={Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Mike},
  booktitle={International Conference on Machine Learning},
  pages={344--353},
  year={2019},
  organization={PMLR}
}

@article{pearl2010causal,
  title={Causal inference},
  author={Pearl, Judea},
  journal={Causality: Objectives and Assessment},
  pages={39--58},
  year={2010},
  publisher={PMLR}
}

@book{glymour2016causal,
  title={Causal Inference in Statistics: A Primer},
  author={Glymour, Madelyn and Pearl, Judea and Jewell, Nicholas P},
  year={2016},
  publisher={John Wiley \& Sons}
}