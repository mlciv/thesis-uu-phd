%%% -*-LaTeX-*-

\chapter{Introduction}
\label{chap:intro}

Human language is essential for human intelligence and arguably our
most powerful tool for learning and transmitting knowledge. With the
advances of computers and the internet, most of the world's knowledge,
such as conversations, scholarly research, factual news, online
education, and private mental health records, is now easily accessible
as digitized text. However, with limited information processing
ability, we cannot easily discover the knowledge hidden in the vast
amount of unstructured text.

One classical way to study unstructured natural language is to
represent the language via various structured symbolic representations
at different levels~\citep{smith2011linguistic}. Before the revolution
of representation learning with deep learning, the NLP community had
put decades of effort into solving other linguistic structured
prediction tasks to get various aspects of text understanding. Let us
look at some examples.

% with pipelined toolkits~\citep{manning2014stanford,bird2004nltk},
\begin{figure}[!th]
\centering
\includegraphics[width=0.85\textwidth]{dog-pos.pdf}
\caption{\label{fig:intro:dog-pos}Part-of-speech tags for the sentence
  \textit{"The dog cannot find the bone it hid from the other dogs."}
  This image shows the tag set used in Penn
  Treebank~\cite{marcus-etal-1994-penn} }
\end{figure}

\begin{figure}[!th]
\centering
\includegraphics[width=0.95\textwidth]{dog-tree.pdf}
\caption{\label{fig:intro:dog-tree}The constituent tree for the sentence \textit{"The dog cannot find the bone it
    hid from the other dogs"}}
\end{figure}

\begin{figure}[!th]
\centering
\includegraphics[width=0.98\textwidth]{dog-dep.pdf}
\caption{\label{fig:intro:dog-dep}The dependency tree, for the
  sentence \textit{"The dog cannot find the bone it hid from the other
    dogs"}}
\end{figure}

\Paragraph{Example 1: Part-of-Speech Tagging, Constituency and
  Dependency Structure} We consider the sentence ``The dog cannot find
the bone it hid from the other dogs" as a running example. As shown
in~\autoref{fig:intro:dog-pos}, the part-of-speech~(\textbf{POS})
tagging assigns each word in a sentence a part-of-speech tag, such as
\lbl{noun}, \lbl{verb},\lbl{adjective}, \lbl{pronoun}. How to capture
the sequential correlations between consecutive tags is the key
modeling challenge for this task.  \autoref{fig:intro:dog-tree} shows
the \textbf{constituent tree} structure of the sentence. The
constituent tree parsing requires recognizing the recursive phrase
structure of a sentence, such as noun, verb, prepositional phrases,
and their nesting in each other. \autoref{fig:intro:dog-dep} shows the
\textbf{dependency tree} structure of the sentence. Unlike the
constituency structure, here the dependency structure of a sentence is
described in terms of the directed bi-lexical grammatical relations
between words. Each labeled arc represents a directed relation from
headwords to their dependents. Besides the above lexical and syntactic
structured information, as shown in the left part of
\autoref{fig:intro:dog-amr}, natural language semantics is also widely
studied as structured representations via tasks such as \textbf{ word
  sense diambiguation}, \textbf{semantic role labeling}
and~\textbf{co-reference resolution} and so on~\footnote{More details
  about various semantic phenomena will be introduced
  in~\autoref{ssec:bg:broad-mr}}. Such structured information is
widely used in classical feature-engineering based NLP
system~\citep[e.g.,][]{Joh:Nug:08,hovy2010s,punyakanok2008importance},
they are still helpful in deep learning based
systems~\citep{moosavi-strube-2018-using,strubell-etal-2018-linguistically,bowman-etal-2016-fast}.

\Paragraph{Example 2: Broad-coverage Meaning Representation} Besides
the above structures capturing specific lexical, syntactic or semantic
information, a broad-coverage semantic representation is a
general-purpose meaning representation language aiming to represent
the multiple phenomena in a single structure for broad-coverage
text. \autoref{fig:intro:dog-amr} shows the Abstract Meaning
Representation~\citep[\textbf{AMR},][]{Ban:Bon:Cai:13} of the example
sentence. The node in the graph represents abstract
concepts~\footnote{AMR concepts include PropBank framesets, and other special dates, spatial entities, etc. More details about AMR will
  be introduced in~\autoref{ssec:bg:broad-mr}}, and the labeled edges
between the nodes represent the relations between those concepts. As
shown in \autoref{fig:intro:dog-tree}, the node `findâ€“01' and
`hide-01' represents the word sense predefined in the
Propbank~\cite{Kin:Pal:02}; The connected edges `:ARG0' and `:ARG1'
captures the semantic roles that can be derived from the semantic role
labelling tasks; While the node `dog/d1' means the subject for events
`find-01', `hide-01' and 'possible-01' are the same dog, thus
capturing the coreference information. \autoref{fig:intro:dog-ucca}
shows the foundational layer of Universal Conceptual Cognitive
Annotation~\citep[\textbf{UCCA},][]{Abe:Rap:13b}, which is a
multi-layered framework for semantic representation that aims to
accommodate the semantic distinctions in the sentence and support
open-ended extensions. Different from AMR, this UCCA foundational
layer mainly forms a tree-like structure, which focuses on the argument
structures of verbal, nominal, and adjectival predicates and the
inter-relations between them. Besides the above two broad-coverage
meaning representations, we also studied the DELPH-IN MRS Bi-lexical
Dependencies~\cite[DM,][]{ivanova2012did} and Prague Semantic
Dependencies~\cite[PSD,][]{hajic2012announcing,miyao2014house}. More
details about their captured semantic content and their structure
properties will be introduced comparatively
in~\autoref{ssec:bg:broad-mr}.

\begin{figure}[!th]
\centering
\includegraphics[width=1.0\textwidth]{broad-coverage-mr.pdf}
\caption{\label{fig:intro:dog-amr} The broad coverage meaning
  representation AMR for the sentence \textit{"The dog cannot find the
    bone it hid from the other dogs."}. It represents multiple
  phenomena in a single structure, inclduing the predicate-argument
  structure and word sense disambiguiation in semantic role labelling,
  coreference resolution, and so on.}
\end{figure}


\begin{figure}[!th]
\centering
\includegraphics[width=0.90\textwidth]{dog-ucca.pdf}
\caption{\label{fig:intro:dog-ucca} The broad coverage meaning
  representation UCCA for the sentence \textit{"The dog cannot
    find the bone it hid from the other dogs."}}
\end{figure}

\Paragraph{Example 3: Application-specific Symbolic Representation}
Besides the above broad coverage syntactic and semantic structures in
natural language, researchers have designed various symbolic
representations for specific applications. Dialogue acts are firstly
designed to represent the speech act or intention of each utterance,
to represent the functions of each utterance in the
dialogue~\citep{wittgenstein2010philosophical,bunt2010towards}. Then
inspired by the case theory~\citep{Fillmore:68}, frame-based
representation in GUS~\citep{bobrow1977gus} are introduced to
represent the state of dialogue, which consists of a collection of
slots and each with a set of possible
values. \autoref{fig:intro:dialogue} shows an example of dialogue state
tracking, where each table is filled with intent, slot, and slot values,
representing a dialogue state for a user turn.

\begin{figure}[!th]
\centering
\includegraphics[width=0.90\textwidth]{dialogue-example.pdf}
\caption{\label{fig:intro:dialogue} Example for dialogue state
  tracking.}
\end{figure}

Lexical, syntactic structures, broad coverage semantic representations
and application-specific representations are interpretable to both
human and computers. Such structured representations can enable
rigorous document analysis, easier knowledge organization, and
programmable reasoning. Furthermore, they can be potentially helpful
to offer actionable suggestions to guide human behavior, such as
improving mental health counseling~\citep{tanana2016comparison},
dialogue state tracking~\citep{budzianowski2018multiwoz}, scientific
document analysis~\citep{dernoncourt2017pubmed}, and so on.

With the stunning rise of deep learning, modern NLP systems have
achieved outstanding performance on many benchmark tasks, and offer
helpful services, such as machine translation. Without any prior
knowledge of the syntax or semantic structures for feature
engineering, they feed a large amount of labeled raw data
into an end-to-end deep learning model and outperform many previous
pipeline models built from hand-crafted features. Recently, pretrained
large language models even became the unified base model for many of
the NLP tasks, which further boosts the performance.

However, recent research has shown that such end-to-end NLP systems
often fail catastrophically when given unseen inputs from different
sources or via adversarial attacks. The end-to-end black-box models
lack interoperability and robustness and they are fragile to maintain
when deployed to real users. Using those large language models without
any careful intervention can lead to fairness
issues~\citep{bommasani2021opportunities}.  Using interpretable
symbolic representation in deep learning models can improve both the
efficiency and robustness of NLP systems. For example, combining the
power of neural representation with symbolic AMR representation has
shown great benefits to NLP applications like machine
translation~\citep{song2019semantic},
summarization~\citep{liu2015toward}, question
answering~\citep{kapanipathi2021leveraging} and so on.

Predicting structured representations of text is essential for natural
language processing, even in the deep learning era. In this thesis, we
ground the studies of natural language structured prediction on both
broad-coverage meaning representations and application-specific
representations. Beyond pure data-driven methods, we primarily study
deep linguistic structured prediction via independent
factorization. We propose two kinds of generic inductive biases to
support the independent factorization for each task, including
Structural Inductive Biases and Natural Language as Inductive Biases.

\input{chapters/intro/intro-motivation}
%\input{chapters/intro/intro-replearning}
\input{chapters/intro/intro-contribution}
\input{chapters/intro/intro-roadmap}

%%% Index phrases should be attached to an important word of a phrase,
%%% and are usually best kept on a separate line by terminating the
%%% previous line with a percent comment without intervening space, as
%%% in this example:
%%%
%%%     \newcommand {\X} [1] {#1\index{#1}}
%%%
%%%     African ungulates,%
%%%     \index{African ungulate}
%%%     like the \X{gnu}, \X{impala}, \X{kudu}, and \X{springbok}
%%%     live mostly in hot climate and consume vegetation.
%%%
%%% However, for this document, we only want lots of index entries to
%%% populate a sample topic index.
\index{Inductive Bias}
\index{Structured Prediction}
\index{Linguistic Structured Prediction}
\index{AMR, Abstract Meaning Representations}
\index{PSD, Prague Semantic Dependencies}
\index{DM, DELPH-IN MRS Bi-lexical Dependencies}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis-main.ltx"
%%% End:
