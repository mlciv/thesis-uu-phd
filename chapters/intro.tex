%%% -*-LaTeX-*-

\chapter{Introduction}
\label{chap:intro}

Human language is essential for human intelligence and arguably our
most powerful tool for learning and transmitting knowledge. With the
advances of computers and the internet, most of the world's knowledge,
such as conversations, scholarly research, factual news, online
education, and private mental health records, is now easily accessible
as digitized text. However, with limited information processing
ability, we cannot easily discover the knowledge hidden in the vast
amount of unstructured text.

One classical way to study unstructured natural language is to
represent the language via various structured symbolic representations
at different levels~\citep{smith2011linguistic}. Before the revolution
of representation learning with deep learning, the NLP community had
put decades of effort into solving other linguistic structured
prediction tasks to get various aspects of text understanding. Let us
look at some examples.

% with pipelined toolkits~\citep{manning2014stanford,bird2004nltk},
\begin{figure}[!tbp]
\centering
\includegraphics[width=0.95\textwidth]{dog-pos.pdf}
\caption{\label{fig:intro:dog-pos}The structure of POS tags for the
  sentence \emph{\dquoted{The dog cannot find the bone it hid from the
      other dogs.}}~This image shows the tag set used in Penn
  Treebank~\cite{marcus-etal-1994-penn}.}
\end{figure}

\begin{figure}[!tbp]
\centering
\includegraphics[width=0.95\textwidth]{dog-tree.pdf}
\caption{\label{fig:intro:dog-tree}The structure of constituent tree
  for the sentence~\emph{\dquoted{The dog cannot find the bone it hid
      from the other dogs.}}}
\end{figure}

\begin{figure}[!tbp]
\centering
\includegraphics[width=0.98\textwidth]{dog-dep.pdf}
\caption{\label{fig:intro:dog-dep}The structure of dependency tree for
  the sentence \emph{\dquoted{The dog cannot find the bone it hid from the
    other dogs.}}}
\end{figure}

Firstly, we illustrate the \kw{classic linguistic structures}, such as
part-of-speech~(POS), constituency and dependency trees. We consider
the sentence \dquoted{The dog cannot find the bone it hid from the
  other dogs.}~as a running example. As shown
in~\autoref{fig:intro:dog-pos}, the part-of-speech tagging assigns
each word in a sentence a part-of-speech tag, such as \lbl{noun},
\lbl{verb},\lbl{adjective}, \lbl{pronoun}. How to capture the
sequential correlations between consecutive tags is the key modeling
challenge for this task.  \autoref{fig:intro:dog-tree} shows the
\kw{constituent tree} structure of the sentence. The constituent tree
parsing requires recognizing the recursive phrase structure of a
sentence, such as noun, verb, prepositional phrases, and their nesting
in each other. \autoref{fig:intro:dog-dep} shows the \kw{dependency
  tree} structure of the sentence. Unlike the constituency structure,
here the dependency structure of a sentence is described in terms of
the directed bilexical grammatical relations between words. Each
labeled arc represents a directed relation from headwords to their
dependents. Besides the above lexical and syntactic structured
information, as shown in the left part of \autoref{fig:intro:dog-amr},
natural language semantics is also widely studied as structured
representations via tasks such as \kw{ word sense diambiguation},
\kw{semantic role labeling} and~\kw{co-reference resolution} and so
on~\footnote{More details about various semantic phenomena will be
  introduced in~\autoref{ssec:bg:broad-mr}}. Such structured
information is widely used in classical feature-engineering based NLP
system~\citep[\eg,][]{Joh:Nug:08,hovy2010s,punyakanok2008importance},
they are still helpful in deep learning based
systems~\citep{moosavi-strube-2018-using,strubell-etal-2018-linguistically,bowman-etal-2016-fast}.

Secondly, we exmaine \kw{broad-coverage meaning
  representations}. Besides the above structures capturing specific
lexical, syntactic or semantic information, a broad-coverage semantic
representation is a general-purpose meaning representation language
aiming to represent the multiple phenomena in a single structure for
broad-coverage text. \autoref{fig:intro:dog-amr} shows the Abstract
Meaning Representation~\citep[\kw{AMR},][]{Ban:Bon:Cai:13} of the
example sentence. The node in the graph represents abstract
concepts~\footnote{AMR concepts include PropBank framesets, and other
  special dates, spatial entities, etc. More details about AMR will be
  introduced in~\autoref{ssec:bg:broad-mr}}, and the labeled edges
between the nodes represent the relations between those concepts. As
shown in \autoref{fig:intro:dog-tree}, the node \tquoted{find-01} and
\tquoted{hide-01} represents the word sense predefined in the
Propbank~\cite{Kin:Pal:02}; The connected edges \tquoted{:ARG0} and
\tquoted{:ARG1} captures the semantic roles that can be derived from
the semantic role labelling tasks; While the node \tquoted{dog/d1}
means the subjects for the events \tquoted{find-01,}~\tquoted{hide-01,}~and
\tquoted{possible-01} are the same dog, thus capturing the coreference
information. \autoref{fig:intro:dog-ucca} shows the foundational layer
of Universal Conceptual Cognitive
Annotation~\citep[\kw{UCCA},][]{Abe:Rap:13b}, which is a multilayered
framework for semantic representation that aims to accommodate the
semantic distinctions in the sentence and support open-ended
extensions. Different from AMR, this UCCA foundational layer mainly
forms a tree-like structure, which focuses on the argument structures
of verbal, nominal, and adjectival predicates with also the
interrelations between them. Besides the above two broad-coverage
meaning representations, we also studied the DELPH-IN MRS Bilexical
Dependencies~\citep[\kw{DM},][]{ivanova2012did} and Prague Semantic
Dependencies~\citep[\kw{PSD},][]{hajic2012announcing,miyao2014house}. More
details about their captured semantic content and their structure
properties will be introduced comparatively
in~\autoref{ssec:bg:broad-mr}.

\begin{figure}[!tbp]
\centering
\includegraphics[width=1.0\textwidth]{broad-coverage-mr.pdf}
\caption{\label{fig:intro:dog-amr} The broad coverage meaning
  representation AMR for the sentence \emph{\dquoted{The dog cannot find the
    bone it hid from the other dogs.}} It represents multiple
  phenomena in a single structure, inclduing the predicate-argument
  structure and word sense disambiguiation in semantic role labelling,
  coreference resolution, and so on.}
\end{figure}


\begin{figure}[!tbp]
\centering
\includegraphics[width=0.90\textwidth]{dog-ucca.pdf}
\caption{\label{fig:intro:dog-ucca} The broad coverage meaning
  representation UCCA for the sentence \emph{\dquoted{The dog cannot
    find the bone it hid from the other dogs.}}}
\end{figure}

Finally, besides the above broad coverage syntactic and semantic
structures in natural language, researchers have designed various
\kw{symbolic representations for specific applications}. Dialogue acts are
firstly designed to represent the speech act or intention of each
utterance, to represent the functions of each utterance in the
dialogue~\citep{wittgenstein2010philosophical,bunt2010towards}. Then
inspired by the case theory~\citep{Fillmore:68}, frame-based
representation in GUS~\citep{bobrow1977gus} are introduced to
represent the state of dialogue, which consists of a collection of
slots and each with a set of possible
values. \autoref{fig:intro:dialogue} shows an example of dialogue
state tracking, where each table is filled with intent, slot, and slot
values, representing a dialogue state for a user turn.

\begin{figure}[!tbp]
\centering
\includegraphics[width=0.98\textwidth]{dialogue-example.pdf}
\caption{\label{fig:intro:dialogue} An example for dialogue state
  tracking.}
\end{figure}

Lexical, syntactic structures, broad coverage semantic representations
and application-specific representations are interpretable to both
human and computers. Such structured representations can enable
rigorous document analysis, easier knowledge organization, and
programmable reasoning. Furthermore, they can be potentially helpful
to offer actionable suggestions to guide human behavior, such as
improving mental health counseling~\citep{tanana2016comparison},
dialogue state tracking~\citep{budzianowski2018multiwoz}, scientific
document analysis~\citep{dernoncourt2017pubmed}, and so on.

With the stunning rise of deep learning, modern NLP systems have
achieved outstanding performance on many benchmark tasks, and offer
helpful services, such as machine translation. Without any prior
knowledge of the syntax or semantic structures for feature
engineering, they feed a large amount of labeled raw data
into an end-to-end deep learning model and outperform many previous
pipeline models built from hand-crafted features. Recently, pretrained
large language models even became the unified base model for many of
the NLP tasks, which further boosts the performance.

However, recent research has shown that such end-to-end NLP systems
often fail catastrophically when given unseen inputs from different
sources or via adversarial attacks. The end-to-end black-box models
lack booth interoperability and robustness, and they are fragile to maintain
when deployed to real users. Using those large language models without
any careful intervention can lead to fairness
issues~\citep{bommasani2021opportunities}.  Using interpretable
symbolic representation in deep learning models can improve both the
efficiency and robustness of NLP systems. For example, combining the
power of neural representation with symbolic AMR representation has
shown great benefits to NLP applications like machine
translation~\citep{song2019semantic},
summarization~\citep{liu2015toward}, question
answering~\citep{kapanipathi2021leveraging} and so on.

Predicting structured representations of text is essential for natural
language processing, even in the deep learning era. In this dissertation, we
ground the studies of natural language structured prediction on both
broad-coverage meaning representations and application-specific
representations. Beyond pure data-driven methods, we primarily study
deep linguistic structured prediction via independent
factorization. We propose two kinds of generic inductive biases to
support the independent factorization for each task, including
structural inductive biases and natural language as inductive biases.

\input{chapters/intro/intro-motivation}
%\input{chapters/intro/intro-replearning}
\input{chapters/intro/intro-contribution}
\input{chapters/intro/intro-roadmap}

%%% Index phrases should be attached to an important word of a phrase,
%%% and are usually best kept on a separate line by terminating the
%%% previous line with a percent comment without intervening space, as
%%% in this example:
%%%
%%%     \newcommand {\X} [1] {#1\index{#1}}
%%%
%%%     African ungulates,%
%%%     \index{African ungulate}
%%%     like the \X{gnu}, \X{impala}, \X{kudu}, and \X{springbok}
%%%     live mostly in hot climate and consume vegetation.
%%%
%%% However, for this document, we only want lots of index entries to
%%% populate a sample topic index.
\index{Inductive Bias}
\index{Structured Prediction}
\index{Linguistic Structured Prediction}
\index{AMR, Abstract Meaning Representations}
\index{PSD, Prague Semantic Dependencies}
\index{DM, DELPH-IN MRS Bilexical Dependencies}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../dissertation-main.ltx"
%%% End:
