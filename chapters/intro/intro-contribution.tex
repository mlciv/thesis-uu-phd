\section[Dissertation Statement and Research
Contributions]{Dissertation Statement and Research \\Contributions}

\label{sec:intro-contri}

Our claim is that by designing ~\kw{structural inductive biases} and
\kw{natural language as inductive biases}, models with naive
independent factorization can achieve strong performance at predicting
the natural language structures across multiple broad-coverage meaning
representations and application-specific representations.

In this dissertation, focusing on the independent factorization
setting, we show that our proposed inductive biases can offer
discriminative features to achieve competitive and generalizable
performance on broad-coverage meaning representations and
application-specific representations.

We next summarize the main contributions of this dissertation, addressing
some of the open problems mentioned in the previous section.

\begin{enumerate}
\item Based on the assumption of independent factorization, we
  proposed a unified parsing framework to support
  both~\kw{explicit lexical-anchoring} (including DELPH-IN MRS
  Bi-lexical Dependencies~\citep[DM,][]{ivanova2012did} and Prague
  Semantic
  Dependencies~\citep[PSD,][]{hajic2012announcing,miyao2014house}),
  and \kw{implicit lexical anchoring}~(AMR). Over 16 teams in the
  shared tasks, my parser~\citep{cao2019amazon} \kw{ranked 1st on AMR,
    6th in DM, and 7th in PSD}. By combining Perturb-and-MAP
  sampling~\citep{papandreouperturb} with differentiable
  Gumbel-Softmax Sinkhorn Networks~\citep{mena2018learning}, we can
  approximately infer the discrete latent-alignment variable in
  lexical-anchoring of the independent factorization setting. The
  \kw{phrasal-anchoring} Universal Conceptual Cognitive
  Annotation~\citep[UCCA,][]{abend2013universal} and Task-oriented
  Dialog Parsing~\citep[TOP,][]{gupta-etal-2018-semantic-parsing} are
  similar to a constituency tree structure, except for unseen phenomena
  such as remote edges and discontinuous spans, we extend the existing
  algorithmic inductive bias for tree structure prediction and
  Cost-augmented CKY inference to the new UCCA and TOP parsing
  tasks. Powered by a strong span-representation learning method, my
  system~\citep{cao2019amazon} \kw{ranked 5/16 on UCCA parsing}, and
  it can be reused for TOP parsing after a few preprocessing steps,
  and outperform several baseline models.

\item To provide real-time guidance to therapists with a dialogue
  observer, we decompose the dialog structure analysis with two
  independent prediction tasks: (1) categorizing therapist and client
  MI behavioral codes and (2) forecasting codes for upcoming
  utterances to help guide the conversation and potentially alert the
  therapist. For both tasks, I studied a hierarchical gated recurrent
  unit (\HGRU) with the \kw{word-level attention} and
  \kw{sentence-level attention} to distinguish different importance of
  words and sentences~\citep{jie2019psycdialacl}. Our experiments
  demonstrate that our models can outperform several baselines for
  both tasks. We also report the results of a careful analysis that
  reveals the impact of the various network design tradeoffs for
  modeling therapy dialogue.

  \item By decomposing the dialogue state tracking into four independent
  sub-tasks, we use natural language description as inductive biases
  to describe the functions of each independent intent/slot label,
  thus capturing the functional overlapping between different
  services. We show that such natural language descriptions can
  support the zero-shot learning for each independent subtask for
  unseen service. We are among the first to use large pretrained
  language models for description-based dialog state tracking. We
  offer detailed comparative studies on how to transfer inductive
  biases to new domains and APIs with overlapping functions and task
  structures, including encoding strategies, supplementary
  pretraining, homogeneous, and heterogeneous evaluations.
\end{enumerate}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../dissertation-main.ltx"
%%% End:
