
\section{Thesis Outline}
\label{sec:intro:roadmap}
In the thesis, we discuss prior work related to an application in its
own chapter, instead of putting them all in a single
chapter. Therefore, we include a section of related work at the end of
each application chapter. This thesis is divided into five parts,
which we described below.

\Paragraph{Chapter 2. Background} The first part systematizes the
background of this thesis study in two sections:
\begin{itemize}
\item \kw{Structures in NLP.} We first provide the necessary
  background about structures in natural languages for better
  highlighting our contributions in remaining chapters.
\item \kw{Structured Prediction, Learning and Inference.} We summarize
  the recent advances in deep structured prediction with respect to
  representational formaliam, learning and inference respectively. We
  overview the development of representation learning methods for
  natural language, from feature selection to deep learning based
  representation learning methods.
\end{itemize}

\Paragraph{Chapter 3. Structural Inductive Biases for Lexical and
  Phrasal Anchoring} In this chapter, we introduce lexical and phrasal
anchoring analysis to decompose the output structures into locally
independent parts, where each part can be derived from its anchoring
words or phrases in the input sentence. For lexical-anchoring, we
propose a unified model to support both explicit and implicit
alignment information between each input and output. For
phrasal-anchoring, we compared different ways to learn the
contextualized representation for the spans, and how they can bring
discriminative features to our locally-dependent model. We show that
with the above lexical and phrasal-anchoring based structural
inductive biases for energey factorization and contextualized
representation learning, our model can learn efficient discriminative
features for the anchor and achieve high performance in the
locally-independent model.

\Paragraph{Chapter 4. Structural Inductive Biases for Sentence and
  Dialog} In the following two chapters, we extend our study to
structures beyond a single sentence. In this chapter, we study the
sequential dialog flow structure in a style of therapy called
Motivational
Interviewing~\cite[MI,][]{miller2003motivational,miller2012motivational},
which is widely used for treating addiction-related problems.
Sentence-level tags called Motivational Interview Skill Codes are
designed to represent the intention of each utterance and the dialogue
flow of the whole therapy session. By developing a modular family of
neural networks categorizing and forcasting the dialog flow in the
form of MISC codes, we show that the above mechanisms on dialogue
representation can efficiently model the sequential structure of
dialogue flow.

\Paragraph{Chapter 5. Natural Language as Inductive Biases} In this
chapter, we study using natural language descriptions to represent the
meaning of output symbols~(Intents and Slots) in task-oriented dialog
state tracking, which helps to reduce the poor scalability to transfer
to unseen domain and services. We study three main challenges of using
natural language for label representation: schema encoding,
supplementary training, and description styles.


\Paragraph{Chapter 6. Conclusion and Future Work} This chapter
concludes, by providing a summary of contributions and a discussion of
possible directions of future work.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../thesis-main.ltx"
%%% End:
