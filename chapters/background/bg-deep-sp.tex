\section{Deep Structured Prediction in NLP}
\label{sec:background:deepsp}
Due to the power of representation learning, deep learning is widely
used to extract sophisticated representations for the inputs in
various NLP tasks. In this proposal, instead of focusing on a single
task, we systematically study the representation learning challenges
for multiple sets of tasks based on different factorization
strategies\S\ref{ssec:sp-anchors}. Besides the representation learning
for sequential input text, we also study the representation of output
structures. It is related to the main research problem for efficient
deep structured inference~(for both training-time and test-time
inference): designing continuous and differentiable relaxations for
\kw{argmax} inference. Most works studied how to inject known
hand-crafted constraints bewteen discrete variable assignments, e.g.,
using differentiable fuzzy logic
operators~\citet{li2019augmenting}. By relaxing the whole structured
outputs into continuous vectors, structured prediction energy
networks~\cite[SPEN,][]{belanger2016structured} define energy
functions that can learn the arbitrary dependencies among parts of
structured outputs. Following this, inference
network~\cite{tu2018learning} was proposed to approximate the
cost-augmented inference during training and then fine-tuning for
test-time inference.

In this thesis, we study the specific inductive biases based on the
principle of compositionality, which can be used for \textbf{different
  purposes more than the accuracy on a single task or a single
  domain}. For example, with simple independent factorization, we
study a universal alignment-based model to support
\textit{cross-framework} meaning representation parsing. By defining
two complementary dialogueue observers to sequentially predict the
MISC code for both current and future utterance, our model emphasizes
both \textit{accurate and real-time} assistance to a therapist. We
propose to represent the output labels with natural language
descriptions for \textit{zero-shot learning} on unseen
labels~\S\ref{ssec:sgdst}. In our thesis, we show that during the
rapid progress of representation learning methods, our proposed
inductive biases still can outperform the standard usage baselines.


\subsection{Formulation of Structural Interdependence}

\subsubsection{Graphic Model}

\subsubsection{Declaritive Interdependence}

\subsubsection{Learning Interdependence}

\subsection{Neural Representation Learning}
For deep learning based methods, we review the recent advances from
embedding based methods to attention-based dynamic features
selection. Finally, we also introduced the rapid progress in language
encoding architectures, from recurrent neural network to transformer,
and the corresponding pretrained language models ELMo , BERT, GPT3
etc.

\subsection{Modeling Discrete Latent Variables}

\subsection{Other Advances in Structured Prediction}
\subsubsection{MAP Inference}
Exact: Dynamic Programming: Viterbi, CKY, Max Spanning Arborescence, Exact Search, Integer Linear Programming
Approximate: Sampling, Beam Search/Learning to Search, Linear Programming Relaxations

\subsubsection{Marginal Inference}
Forward-Backward/ Inside-outside/ Matrix tree
CRF
SparseMAP

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../thesis-main.ltx"
%%% End:
