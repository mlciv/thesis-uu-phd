\section{Deep Ligusitic Structured Prediction and Independent Factorization}
\label{sec:background:deepsp}
Due to the power of representation learning, deep learning is widely
used to extract sophisticated representations for the inputs in
various NLP tasks. In this thesis, instead of focusing on a single
task, we systematically study the representation learning challenges
for multiple sets of tasks based on independent factorization
assumption.  In this section, We first introduce the recent advances
in deep structured prediction with respect to representational
formalism~\S\ref{ssec:bg:formalism} and introduce the independent
factorization with factor graphs. Then we briefly summarize the
progress on the representation learning on natural
language~\S\ref{ssec:bg:rep-learning} and show why the independent
factorization is possible with the contextualized representation.
Finally, we also summerize the recent advances on
inference~\S\ref{ssec:bg:inference} on lingustic structured prediction.

%In this thesis, based on the independent factorization, we study the
%generic inductive biases for \textbf{different purposes more than the
%  accuracy on a single task or a single domain}. For example, by
%exploiting the known linguistic knowledge about anchoring, we study
%five \textit{cross-framework} meaning representation parsing with a
%lexical-anchoring parser and a . By defining two complementary
%dialogueue observers to sequentially predict the MISC code for both
%current and future utterance, our model emphasizes both
%\textit{accurate and real-time} assistance to a therapist. We propose
%to represent the output labels with natural language descriptions for
%\textit{zero-shot learning} on unseen labels~\S\ref{chap:sgd}. In our
%thesis, we show that during the rapid progress of representation
%learning methods, our proposed inductive biases still can outperform
%the standard usage baselines.

\subsection{Formulation of Structural Interdependence}
\label{ssec:bg:formalism}

\Paragraph{Graphical Models} A graphical model is a probabilistic
model for which a graph expresses the conditional dependence structure
between random variables. Generally, probabilistic graphical models
use a graph-based representation as the foundation for encoding a
distribution over a multi-dimensional space, which represents a set of
independences that hold in the specific distribution. Two branches of
graphical representations of distributions are commonly used, namely,
Bayesian networks and Markov random fields. Both families encompass
the properties of factorization and independences, but they differ in
the set of independences they can encode and the factorization of the
distribution that they induce.

\Paragraph{Constrained Conditional Models} Besides using graphical
models to declaritively represent the structural interdependence
between variables, constrained conditional
models~\citep[CCM,][]{chang2012structured} is another machine learning
and inference framework for the same goals. More specifically, CCM
emphasizes augmenting the learning of conditional models with
declarative constraints. It aims to support constrained decisions in
an expressive output space while maintaining modularity and
tractability of training and inference. These constraints can express
either hard restrictions, completely prohibiting some assignments, or
soft restrictions, penalizing unlikely assignments. One popular
formalism to represent the constraints is to use an interger linear
programming~(ILP), which has been widely used to constrain learning in
many NLP tasks~\citep{roth2007global}. The declaritive linear
objective functions and linear constraints, and the availibities of
the off-the-shelf solvers make this formalism very easy to use.

\Paragraph{Declaring Constraints in Deep Learning} Recently, to inject
known hand-crafted constraints bewteen discrete variable assignments
in the deep neural networks, one foundmental challange is how to
represent the constraints in end-to-end differentiable
ways~\cite{bach2017hinge}. For example, \citet{li2019augmenting}
propose to use differentiable fuzzy logic operators to augment the
neural networks with boolean logic. \citet{pacheco2021modeling}
introduces a declarative Deep Relational Learning framework~(\DRAIL)
via integrating neural representation learners with probabilistic
logic. Besides representing the constraints as logic forms, many
recent work also studies representing constraints with discrete latent
variable models, such as StructVAE for latent tree structured
variables, ~\cite{yin2018structvae, corro2019learning}. Our work on
latent alignment models also falls into this category.

\Paragraph{Learning Constraints in Deep Learning} Besides injecting
declaritive constraints, recent research also learn the constraints in
an end-to-end ways. \citet[SPEN,][]{belanger2016structured} define
energy functions that can learn the arbitrary dependencies among parts
of structured outputs by relaxing the whole structured outputs into
continuous vectors, Following this, inference
network~\cite{tu2018learning} was proposed to learn the constrained
network for inference, which approximate the cost-augmented inference
during training and then fine-tuning for test-time inference.

\Paragraph{Independent Factorization} In this thesis, we mainly use
the undirected Markov random fileds to represent our independent
factorization assumptions. As shown
in~\autoref{fig:bg:graphical-model}, each circle represent a variable,
while each rectangle shows a factor between the input sentence
variable $x$ and each decomposed segements of output structures
$y$. The difference between left and right figure lies to the
alignment variable $a$ in the center. In the left figure, the shade
cicle $a$ means the alignment are explicitly observed after the
decomposion. While in the right figure, the alignment variable $a$ is
not observed.

\begin{figure}[!th]
\centering
\includegraphics[width=0.90\textwidth]{graphical-models.pdf}
\caption{\label{fig:bg:graphical-model}The factor representation for
  the independent factorization used in our thesis}
\end{figure}

As discuessed in \autoref{ssec:intro:bias-dsp}, to apply independent
factorization for each task, we need to resolve three main challenges
to formulate the above factor graph as
$E(x, y) = \sum_{c \in C}E(x, a(y_{c}), y_{c})$, including
\begin{inparaenum}[(1)]
\item \textbf{Output Decomposition:}~Decomposing the output $y$ into a set of independent parts
  $y_{c}$.
\item \textbf{Input Decomposition and Alignments Discovery:}~Decomposing $x$ and derive the aligned input $x_{a_{y_{c}}}$ at
  the index $a_{y_{c}}$.
\item \textbf{Factor Modelling:}~Modelling each $y_{c}$ and its
  relevant parts $x_{a_{y_{c}}}$ to compute the energy score
  $E(x, a(y_{c}), y_{c})$.
\end{inparaenum}

In the following subsection, we show that rapid progress in
contextualized representation learning can offer discriminative
features for modelling the relative parts of $x_{a_{y_{c}}}$, thus
make the 3rd challenge of independent factorization possible. Then we
provide the background knowledge about structures in NLP in
\autoref{sec:bg:symbolic}, and we briefly show that such prior
knowledge about anchoring and the compositionality are the main source
of inductive biases that guide us to find the decomposition and
alignment in the challenge 1 and 2.  We extend the detailed analysis
about independent factorization in each application chapter.

\subsection{Neural Representation Learning}
\label{ssec:bg:rep-learning}
Structured prediction requires the representation learning can capture
both the distriminitive interactions between $x$ and $y$ and also
allow efficient combinatorial optimization over $y$. Ideally, we hope
neural repsentation learning can handle all of this.

The key challenge of trying to apply analytical and computational
methods to text data is how to represent the text in a way that is
amenable to operations such as similarity, composition, etc. Besides
the early day one-hot representation and TF-IDF extensions, word
embedding and nerual contextualized representation are widely used in
modern deep learning based models. In this section, we review the
recent advances from static word embedding based methods to
attention-based dynamic features selection and contextualized
representation. Finally, we also introduced the rapid progress in
language encoding architectures, from recurrent neural network to
transformer, and the corresponding pretrained language models ELMo ,
BERT, GPT3 etc.

\Paragraph{Static Word Embedding}
Word embeddings are
commonly~\cite{Baroni:2014,pennington2014glove,li2015generative}
categorized into two types, depending upon the strategies used to
induce them:
\begin{inparaenum}[(1)]
\item Prediction-based models, via local data in sentence~(a word'
  context).
\item Count-based models, via the global corpus-wide statistics~(such
  as word counts, co-occurrece).
\end{inparaenum}
Skip-gram with negative sampling~\cite[SGNS,][]{mikolov13w2v} and
GloVe~\cite{pennington2014glove} are among the best known models for
the two type respectively. However, they create a single fixed
representation for each word, a notable problem with static word
embeddings is that all senses of a polysemous word must share a single
vector.

\Paragraph{Contetualized Representation and Contextualing Models}
To resolve the issue of static word embedding, sequence encoders~(such
as LSTM~\citep{hochreiter97lstm}, Transformer~\citep{NIPS2017_7181})
can be used as contextualizing models to encode the whole context and
produce a \kw{contexualized representation} for each word, phrase or
the whole sentence. In this way, the contextualized representation
dynamicly depends on the entire sentence.  Further more, based on the
neural sequence encodingarchitectures, pretraining language models
with a large amount of text can create more powerful representations.
ELMo~\cite{elmo} creates contextualized representations of each token
by concatenating the internal states of a 2-layer biLSTM trained on a
bidirectional language modelling task. In contrast,
BERT~\citep{devlin2019bert} and GPT-2~\citep{radford2018improving} are
bi-directional and uni-directional transformer-based language models
respectively. \citet{peters2019tune} shows that ELMo contextualized
representation is more suitable to be used as a fixed word embedding,
as also shown in our thesis~\autoref{chap:lexical-phrasal} and
~\autoref{chap:sentence}. While for BERT and GPT-2, finetuning them on
downsteam task will lead better performance, which also inspired us on
exploiting natural language description to understand each output
components~(such as intent, slot labels)~in task-oriened
dialogue~\autoref{chap:sgd}

\subsection{Inference}
\label{ssec:bg:inference}
Learning with structured data typically involves searching or summing
over a set with an exponential number of structured elements, for
example the set of all parse trees for a given sentence. In the deep
learning community, it is common to fit models by computing point
estimates, such as the MLE or MAP estimate. Such MAP inference
approaches seem particularly appealing, since they are computationally
fairly cheap, and can use the prior to reduce overfitting. In this
way, the nerual models only learn a single set of parameter. However,
the point estimation does not capture the associated
uncertainties~\citep{murphy2022probabilistic,wilson2020bayesian}.
Hence, in structured prediction research, we care about both MAP
inference and marginal inference.

\Paragraph{MAP Inference} Various exact inference methods are proposed
for MAP inference in NLP tasks.  Exact inference methods include
dynamic programming based methods~(such as
viterbi~\citep{viterbi1967error} for hidden markov models, CKY for
context-tree
grammars~\cite{kasami1966efficient,younger1967recognition,cocke1969programming}
, Max Spanning Arborescence for spanning
tree~\cite{chu1965shortest,edmonds1967optimum}, and so on), and
Integer Linear
Programming~\citep{roth2005integer,roth2007global,berant2014modeling}. On
the other side, approximate inference methods includes various
sampling methods~\citep{finkel2005incorporating,singh2012monte},
search-based
methods~\citep{daume2009search,ross2011reduction,chang2015learning},
and Linear Programming
Relaxations~\citep{rush2012tutorial,werner2014power}.

\Paragraph{Marginal Inference} Integration is at the heart of marginal
inference, whereas differentiation is at the heart of
optimization. Corresponding to the each of the above exact MAP
inference algorithms, various methods are proposed for marginal
inference. They compute marginal probabilities and partition functions
which are central to many methods, such as
EM~\citep{baker1979trainable,weizenbaum1966eliza}, constrative
estimation~\citep{smith2005contrastive}, Conditional Random
Field~\citep[CRF,][]{lafferty01crf}, max-margin training over all
candidate targets~\cite{koller2004max}. For linguistic structured
prediction, exact marginal inference methods include forward-backward
algorithm for HMM~\citep{binder1997space},
Inside-outside~\citep{baker1979trainable}, Matrix-Tree Theorem for
non-projective dependency
structures~\citep{koo-etal-2007-structured,liu2018learning}.

In this thesis, we use variational inference to marginalize out the
latent alignment variable in
~\autoref{ssec:lex-phr:latent-alignment}. While for the other parts of
this thesis, the assumption of independent factorization simplified
the inference into either greedy~(\S\ref{chap:snt} and
\S\ref{chap:sgd}) or dynamic programming based exact MAP
inference~(such as the dynamic programming parsing the dependency and
constituency structures in~\autoref{sec:lex-phr:two-stage} and
~\autoref{sec:lex-phr:cky-based} respectively)

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../thesis-main.ltx"
%%% End:
