\section{Deep Structured Prediction in NLP}
\label{sec:background:deepsp}
Due to the power of representation learning, deep learning is widely
used to extract sophisticated representations for the inputs in
various NLP tasks. In this thesis, instead of focusing on a single
task, we systematically study the representation learning challenges
for multiple sets of tasks based on independent factorization
assumption.  In this section, We summerize the recent advances in deep
structured prediction with respect to representational
formalism~\S\ref{ssec:bg:formalism},
learning~\S\ref{ssec:bg:rep-learning} and
inference~\S\ref{ssec:bg:inference} respectively.

In this thesis, we study the specific inductive biases based on the
principle of compositionality, which can be used for \textbf{different
  purposes more than the accuracy on a single task or a single
  domain}. For example, with simple independent factorization, we
study a universal alignment-based model to support
\textit{cross-framework} meaning representation parsing. By defining
two complementary dialogueue observers to sequentially predict the
MISC code for both current and future utterance, our model emphasizes
both \textit{accurate and real-time} assistance to a therapist. We
propose to represent the output labels with natural language
descriptions for \textit{zero-shot learning} on unseen
labels~\S\ref{chap:sgd}. In our thesis, we show that during the
rapid progress of representation learning methods, our proposed
inductive biases still can outperform the standard usage baselines.

\subsection{Formulation of Structural Interdependence}
\label{ssec:bg:formalism}

\Paragraph{Graphical Models} A graphical model is a probabilistic
model for which a graph expresses the conditional dependence structure
between random variables. Generally, probabilistic graphical models
use a graph-based representation as the foundation for encoding a
distribution over a multi-dimensional space, which represents a set of
independences that hold in the specific distribution. Two branches of
graphical representations of distributions are commonly used, namely,
Bayesian networks and Markov random fields. Both families encompass
the properties of factorization and independences, but they differ in
the set of independences they can encode and the factorization of the
distribution that they induce.  In this thesis, we mainly use the
undirected Markov random fileds to represent our independent
factorization assumptions.

As shown in~\autoref{fig:bg:graphical-model}, each circle represent a
variable, while each rectangle shows a factor between the input
sentence variable $x$ and each decomposed segements of output
structures $y$. The difference between left and right figure lies to
the alignment variable $a$ in the center. In the left figure, the
shade cicle $a$ means the alignment are explicitly observed after the
decomposion. While in the right figure, the alignment variable $a$ is
not observed.

\begin{figure}[!th]
\centering
\includegraphics[width=0.90\textwidth]{graphical-models.pdf}
\caption{\label{fig:bg:graphical-model}The factor representation for
  the independent factorization used in our thesis}
\end{figure}

\Paragraph{Constrained Conditional Models} Besides using graphical
models to declaritively represent the structural interdependence
between variables, constrained conditional
models~\citep[CCM,][]{chang2012structured} is another machine learning
and inference framework for the same goals. More specifically, CCM
emphasizes augmenting the learning of conditional models with
declarative constraints. It aims to support constrained decisions in
an expressive output space while maintaining modularity and
tractability of training and inference. These constraints can express
either hard restrictions, completely prohibiting some assignments, or
soft restrictions, penalizing unlikely assignments. One popular
formalism to represent the constraints is to use an interger linear
programming~(ILP), which has been widely used to constrain learning in
many NLP tasks~\citep{roth2007global}. The declaritive linear
objective functions and linear constraints, and the availibities of
the off-the-shelf solvers make this formalism very easy to use.

Recently, to inject known hand-crafted constraints bewteen discrete
variable assignments in the deep neural networks, one foundmental
challange is how to represent the constraints in end-to-end
differentiable ways~\cite{bach2017hinge}. For example,
\citet{li2019augmenting} propose to use differentiable fuzzy logic
operators to augment the neural networks with boolean
logic. \citet{pacheco2021modeling} introduces a declarative Deep
Relational Learning framework~(\DRAIL) via integrating neural
representation learners with probabilistic logic.

Besides representing the constraints as logic forms, many recent work
also studies representing constraints with discrete latent variable
models, such as StructVAE for latent tree structured variables,
~\cite{yin2018structvae, corro2019learning}. Our work on latent
alignment models also falls into this category.

Besides injecting declaritive constraints, recent research also learn
the constraints in an end-to-end
ways. \citet[SPEN,][]{belanger2016structured} define energy functions
that can learn the arbitrary dependencies among parts of structured
outputs by relaxing the whole structured outputs into continuous
vectors, Following this, inference network~\cite{tu2018learning} was
proposed to learn the constrained network for inference, which
approximate the cost-augmented inference during training and then
fine-tuning for test-time inference.

\subsection{Neural Representation Learning}
\label{ssec:bg:rep-learning}
Structured prediction requires the representation learning can capture
both the distriminitive interactions between $x$ and $y$ and also
allow efficient combinatorial optimization over $y$. Ideally, we hope
neural repsentation learning can handle all of this.

The key challenge of trying to apply analytical and computational
methods to text data is how to represent the text in a way that is
amenable to operations such as similarity, composition, etc. Besides
the early day one-hot representation and TF-IDF extensions, word
embedding and nerual contextualized representation are widely used in
modern deep learning based models. In this section, we review the
recent advances from static word embedding based methods to
attention-based dynamic features selection and contextualized
representation. Finally, we also introduced the rapid progress in
language encoding architectures, from recurrent neural network to
transformer, and the corresponding pretrained language models ELMo ,
BERT, GPT3 etc.

\Paragraph{Static Word Embedding}
Word embeddings are
commonly~\cite{Baroni:2014,pennington2014glove,li2015generative}
categorized into two types, depending upon the strategies used to
induce them:
\begin{inparaenum}[(1)]
\item Prediction-based models, via local data in sentence~(a word'
  context).
\item Count-based models, via the global corpus-wide statistics~(such
  as word counts, co-occurrece).
\end{inparaenum}
Skip-gram with negative sampling~\cite[SGNS,][]{mikolov13w2v} and
GloVe~\cite{pennington2014glove} are among the best known models for
the two type respectively. However, they create a single fixed
representation for each word, a notable problem with static word
embeddings is that all senses of a polysemous word must share a single
vector.

\Paragraph{Contetualized Representation and Contextualing Models}
To resolve the issue of static word embedding, sequence encoders~(such
as LSTM~\citep{hochreiter97lstm}, Transformer~\citep{NIPS2017_7181})
can be used as contextualizing models to encode the whole context and
produce a \kw{contexualized representation} for each word, phrase or
the whole sentence. In this way, the contextualized representation
dynamicly depends on the entire sentence.  Further more, based on the
neural sequence encodingarchitectures, pretraining language models
with a large amount of text can create more powerful representations.
ELMo~\cite{elmo} creates contextualized representations of each token
by concatenating the internal states of a 2-layer biLSTM trained on a
bidirectional language modelling task. In contrast,
BERT~\citep{devlin2019bert} and GPT-2~\citep{radford2018improving} are
bi-directional and uni-directional transformer-based language models
respectively. \citet{peters2019tune} shows that ELMo contextualized
representation is more suitable to be used a fixed word embedding. In
our thesis, we simply replace the original fixed word embedding with
ELMo, and then feed them into other nerual models. While for BERT and
GPT-2, finetuning them on downsteam task will lead better performance.

\subsection{Inference}
\label{ssec:bg:inference}
Learning with structured data typically involves searching or summing
over a set with an exponential number of structured elements, for
example the set of all parse trees for a given sentence. In the deep
learning community, it is common to fit models by computing point
estimates, such as the MLE or MAP estimate. Such MAP inference
approaches seem particularly appealing, since they are computationally
fairly cheap, and can use the prior to reduce overfitting. In this
way, the nerual models only learn a single set of parameter. However,
the point estimation does not capture the associated
uncertainties~\citep{murphy2022probabilistic,wilson2020bayesian}.
Hence, in structured prediction research, we care about both MAP
inference and marginal inference.

\Paragraph{MAP Inference} Various exact inference methods are proposed
for MAP inference in NLP tasks.  Exact inference methods include
dynamic programming based methods~(such as
viterbi~\citep{viterbi1967error} for hidden markov models, CKY for
context-tree
grammars~\cite{kasami1966efficient,younger1967recognition,cocke1969programming}
, Max Spanning Arborescence for spanning
tree~\cite{chu1965shortest,edmonds1967optimum}, and so on), and
Integer Linear
Programming~\citep{roth2005integer,roth2007global,berant2014modeling}. On
the other side, approximate inference methods includes various
sampling methods~\citep{finkel2005incorporating,singh2012monte},
search-based
methods~\citep{daume2009search,ross2011reduction,chang2015learning},
and Linear Programming
Relaxations~\citep{rush2012tutorial,werner2014power}.

\Paragraph{Marginal Inference} Integration is at the heart of marginal
inference, whereas differentiation is at the heart of
optimization. Corresponding to the each of the above exact MAP
inference algorithms, various methods are proposed for marginal
inference. They compute marginal probabilities and partition functions
which are central to many methods, such as
EM~\citep{baker1979trainable,weizenbaum1966eliza}, constrative
estimation~\citep{smith2005contrastive}, Conditional Random
Field~\citep[CRF,][]{lafferty01crf}, max-margin training over all
candidate targets~\cite{koller2004max}. For linguistic structured
prediction, exact marginal inference methods include forward-backward
algorithm for HMM~\citep{binder1997space},
Inside-outside~\citep{baker1979trainable}, Matrix-Tree Theorem for
non-projective dependency
structures~\citep{koo-etal-2007-structured,liu2018learning}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../thesis-main.ltx"
%%% End:
