\section{Chapter Summary}
\label{sec:conclusion}
In this chapter, beyond the independent factorization and the
attention mechansiam in previous sentential anchroing modelling, we
show that natural language description can further offer
discriminative features to our factor modelling. Esepcially, these
descriptions can offer effective knowledge sharing across different
services, \eg, connecting semantically similar concepts across
heterogeneous APIs, thus allowing a unified model to handle unseen
services and APIs in data-poor cases.

We studied three questions on schema-guided dialog state tracking:
encoder architectures, impact of supplementary training, and effective
schema description styles.  The main findings are as follows:

By caching the token embedding instead of the single \textsc{cls}
embedding, a simple partial-attention \FE can achieve much better
performance than \DE, while still infers two times faster than \CE.
We quantified the gain via supplementary training on two intermediate
tasks.  By carefully choosing representative description styles
according to recent works, we are the first of doing both
homogeneous/heterogeneous evaluations for different description style
in schema-guided dialog. The results show that simple name-based
descriptions perform well on \IC~and \RSI~tasks. On the other side, \NSL~tasks
benefits from richer styles of descriptions.  All tasks suffer from
inconsistencies in description style between training and test, though
to varying degrees.

Our study are mainly conducted on two datasets: \sgdst~and~\multiwoz,
while the speed-accuracy balance of encoder architectures and the
findings in supplementary training are expected to be
dataset-agnostic, because they depend more on the nature of the
subtasks than the datasets. Based on our proposed benchmarking
descriptions suite, the homogeneous and heterogeneous evaluation has
shed the light on the robustness of cross-style schema-guided dialog
modeling, we believe our study will provide useful insights for future
research. Natural language description can also be used for other
tasks where the natural language can be used to describe the
overlapping functionalities and differences. However, how to efficient
design the natural language description will be a challenge problem,
recent ideas on prompt-tuning may potentially help in the
future~\citep{schucher2022power,ye2022ontology}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../dissertation-main.ltx"
%%% End:
