
\section[Independent Factorization for Dialogue State Tracking]{Independent Factorization for \\Dialogue State Tracking}
\label{sec:sgd:ind-factorization}
A classic dialogue state tracker predicts a dialogue state frame at each
user turn given the dialogue history and predefined domain ontology. As
shown in Figure \ref{fig:schema-dst}, the key difference between
schema-guided dialogue state tracking and the classic paradigm is the
newly added natural language descriptions. Beyond the classic
independent factorization, we also add a natural language description
to describe each output labels. Hence, the independent factorization
for schema-guided dialogue state tracking can be represented
as~\autoref{eq:desc-factor}. Instead of using the label of decomposed
part $y_{c}$ only, we also use the natural language description of
$y_{c}$.
\begin{equation}
    \label{eq:desc-factor}
    \begin{split}
    E(x, y) & =\sum_{c \in C} E(x, y_{c}) = \sum_{c \in C}E(x, a(y_{c}), desc(y_{c}))  \\
    \end{split}
\end{equation}

In this section, we will
analyze the task structure of dialogue state tracking and the design
for independent factorization. First, we will examine the
decomposition of dialogue state, and split the dialogue state tracking
into four independent subtasks. In addition to the output
decomposition, we propose to add a natural language description for
each part of the output label to support unseen new services. We
introduce the schema components in schema-guided dialogue state
tracking. Then, we show how each output part can be derived from the
input decomposition. Finally, we outline the research questions in
our paper and address each of them in the rest sections of this
chapter.

\subsection{Output Decomposition: Four Subtasks}
\label{sec:sgd:decompose-y}
As shown in Figure \ref{fig:schema-dst}, the dialogue state for each
service consists of 3 parts: {\it active intent}, {\it requested
  slots}, {\it user goals~(slot values)}. Without loss of generality,
for both \sgdst and \multiwoz datasets, we divide their slots into
categorical and noncategorical slots by following previous study on
dual-strategies~\citep{zhang2019find}. Thus to fill the dialogue state
frame for each user turn, we solve four \kw{independent} subtasks:
intent classification~(\IC), requested slot identification~(\RSI),
categorical slot labeling~(\CSL), and noncategorical slot
labeling~(\NSL).

Beyond the independent factorization of 4 subtasks, we also consider
describing each decomposed intent/slot label with natural
language. By using natural language description to define a dynamic
set of service schema. As shown in Figure \ref{fig:schema-dst}, we
hope these descriptions can connect semantically similar concepts
across heterogeneous APIs, thus allowing a unified model to handle
unseen services and APIs. Hence, in this independent factorization, we
require matching the current dialogue history with candidate schema
descriptions for each subtask and multiple times. Such matches are
computation-heavy, which raises new challenges to our modeling.

Figure \ref{fig:schema-dst} shows three
main schema components: service, intent, slot. For each intent, the
schema also describes {\it optional} or {\it required} slots for
it. For each slot, there are flags indicating whether it is
categorical or not. {\it Categorical} means a set of
predefined candidate values~(\ie, boolean, numeric or text). For instance,
\tquoted{has\_live\_music} in~\autoref{fig:schema-dst} is a categorical
slot with boolean values. {\it Noncategorical}, on the other hand,
means the slot values are filled from the string spans in the dialogue
history.

\subsection[Input Decomposition and Alignments Discovery: Attention]{Input Decomposition and Alignments Discovery: \\Attention Mechanism}
\label{sec:sgd:decompose-x}
For lexical-anchoring representations, the input sentence is decomposed exclustively
into tokens and special entities. For phrasal-anchoring representations, the input
sentence is decomposed into nested phrases which forms a tree
structure. For sentential-anchoring representsions, we show that the input dialogue
are naturally decomposed into a sliding dialogue history window. The
input decomposition for dialogue state tracking is the same as
sentential-anchoring MISC prediction tasks. Because for both MISC
prediction and dialogue state tracking tasks, we cannot simply find
relevant parts to predict the intent and slots, which requires jointly
considering the current dialogue utterance and the previous dialogue
history. Hence, due to the success of attention mechanisms in the MISC
prediction, we also use the attention mechanism to discover the more
detailed relevant parts for our dialogue state tracking. We ground our
study on the pretrained transformer model:
BERT~\citep{devlin2019bert}. BERT uses the self-attention mechanism to
dynamically learn the relevant parts from a large amount of text,
which has been shown great success for many NLP tasks, including
dialogue state tracking~\citep{chao2019bert,noroozi2020fast}.

\subsection{New Questions}
\label{ssec:sgd:new-questions}
These added schema descriptions pose the following three new
questions. We discuss each of them in the following sections.

\begin{itemize}
\item How should dialogue and schema be encoded?(\autoref{sec:sgd:models})
\item How do different supplementary trainings impact each subtask?~(\autoref{sec:sgd:sup-training})
\item How do different description styles impact the state tracking performance?~(\autoref{sec:sgd:abl-desc})
\end{itemize}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../dissertation-main.ltx"
%%% End:
