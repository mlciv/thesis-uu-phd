\section[Phrasal-Anchoring: Minimal Span-based CKY Parsing]{Phrasal-Anchoring: Minimal Span-Based \\CKY Parsing}
\label{sec:lex-phr:cky-based}

Let us now see our phrasal-anchoring parser for UCCA and TOP. We
introduce the transformation~(\autoref{ssec:phr:graph-ct}) used to
reduce UCCA and TOP parsing into a unified constituent tree parsing
task. Then based on the above transformation, we introduce how to do
independent factorization on phrasal-anchoring by addressing the three
main challenges in \textbf{Output
  Decomposition}~(\autoref{sssec:lex-phr:phr-output-decomposition}),
\textbf{Input Decomposition and Alignments
  Discovery}~(\autoref{sssec:lex-phr:phr-input-decomposition}), and
\textbf{Factor Modelling}, including CKY
parsing~(\autoref{ssec:phr:cky}) and span representation
~(\autoref{ssec:phr:span})

\input{chapters/lexical-phrasal/phr-transformation.tex}

\input{chapters/lexical-phrasal/phr-factorization.tex}

\subsection{A Unified Span-based Model for CKY Parsing}
\label{ssec:phr:cky}
After transforming the UCCA graph into a constituent tree, we reduce
the UCCA parsing into a constituent tree parsing problem. Similar to
the previous work on UCCA constituent tree
parsing~\cite{jiang2019hlt}, we use a minimal span-based CKY parser
for constituent tree parsing.  The intuition is to use dynamic
programming to recursively split the span of a sentence recursively,
as shown in~\autoref{fig:ucca-to-CT}. The entire sentence can be
splitted from top to bottom until each span is a single unsplittable
tokens. For each node, we also need to assign a label. Two simplified
assumptions are made when predicting the hole tree given a
sentence. However, different with previous work, we use 8-layers with
8 heads transformer encoder, which shows better performance than LSTM
in \citet{kitaev2018constituency}.

\subsubsection{Tree Factorization}
\label{sssec:phr:tree-factorization}
In the graph-to-tree transformation, we move the edge label to its
child node. By assuming the labels for each node are independent, we
factorize the tree structure prediction as independent span-label
prediction as Equation \ref{eq:tree_factorize}. However, this
assumption does not hold for UCCA.  Please see more error analysis
in~\autoref{ssec:error_breakdown}

\begin{equation}
  \label{eq:tree_factorize}
 \begin{aligned} T^{*}  & = \underset{T}{\textbf{arg\,max}} s(T)     \\ s(T)
                        & = \sum_{(i,j,l) \in T} s(i,j,l)
\end{aligned}
\end{equation}

\subsubsection{CKY Parsing}
\label{sssec:phr:cky-parsing}
By assuming the label prediction is independent of the splitting
point, we can further factorize the whole tree as the following
dynamic programming in Equation~\ref{eq:tree_chart}.

\begin{equation}
  \label{eq:tree_chart}
\begin{aligned}
      s_{\text{best}}(i, i+1) & = \underset{l}{\textbf{max}} s(i,i+1, l) \\
      s_{\text{best}}(i, j) & = \underset{l}{\textbf{max}} s(i,j, l)
      \\ & + \underset{k}{\textbf{max}}[ s_{\text{best}}(i,k) +
      s_{\text{best}}(k,j) ]
\end{aligned}
\end{equation}

\subsection{Span Encoding}
\label{ssec:phr:span}

For each span $(i,j)$, we represent the span encoding vector
$v_{(i,j)} = [\vec{y_{j}} - \vec{y_{i}}] \oplus [\cev{y_{j+1}} -
\cev{y_{i+1}}] $. Here, $\oplus$ denotes vector concatenation. Assuming a
bidirectional sentence encoder, we use the forward and backward
encodings $\vec{y_{i}}$ and $\cev{y_{i}}$ of $i_{th}$ word. Following
the previous work, and we also use the loss augmented inference
training. More details about the network
architecture are in~\autoref{ssec:exp_setup}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../dissertation-main.ltx"
%%% End:
