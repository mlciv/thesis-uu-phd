\section{Experiments and Results}
\label{sec:lex-phr:exp}
This section will first introduce the dataset and the evaluation used
in our experimental study~(\autoref{ssec:data_eval}). Then we
summarize the implementation details when using our proposed
frameworks for each parsing task~(\autoref{ssec:impl_summary}), with
respect to the modeling on root, node, and edge. Then we introduce the
model setup~(\autoref{ssec:exp_setup}), including sentence encoders,
embedding usage, and training strategies. Finally, we demonstrate the
experimental results with both the evaluation
results~(\autoref{ssec:results}) and detailed error
analysis~(\autoref{ssec:error_breakdown}).

\subsection{Dataset and Evaluation}
\label{ssec:data_eval}

For DM, PSD, we split the training set by taking WSJ section (00-19)
as training, and section 20 as dev set. For other datasets, when
developing and parameter tuning, we use splits with a ratio of
25:1:1. In our submitted model, we did not use multitask learning for
training. Following the unified MRP metrics in the shared tasks, we
train our model based on the development set and finally evaluate on
the private test set.  For more details of the metrics, please refer
to the summarization of the MRP~2019 task~\cite{Oep:Abe:Haj:19},
\begin{table}[!tbp]
\caption{The detailed lex-anchoring classifiers in our model. The
  round bracket means the number of ouput classes of our classifiers,
  * means copy mechanism is used in our classifier. }
\label{tbl:lex:impl_lex}
\begin{center}
\begin{tabular}{l|ccc}
\toprule
\hline
                                 & \multicolumn{3}{c}{{\bf Lexicon Anchoring}}                         \\ \cline{2-4}
                                 & DM                    & PSD               & AMR                     \\ \hline
Root                             & 1                     & $ \geq 1$ (11.56\%)  & 1                       \\ \hline
Node Label                       & Lemma                 & Lemma(*)          & Lemma(*) + NeType(143+) \\ \hline
\multirow{2}{*}{Node Properties} & POS                   & POS               & constant values         \\
                                 & SEM-I(160*)\_args(25) & wordid\_sense(25) & polarity, Named entity  \\ \hline
Edge Label                       & (45)                  & (91)              & (94+)                   \\ \hline
Edge Properties                  & N/A                   & N/A               & N/A                     \\ \hline
Connectivity                     & True                  & True              & True                    \\ \hline
Training Data                    & 35656                 & 35656             & 57885                   \\ \hline
Test Data                        & 3269                  & 3269              & 1998                    \\ \hline \bottomrule
\end{tabular}
\end{center}
\end{table}


\begin{table}[!tbp]
\caption{The detailed lex-anchoring classifiers in our model. The round bracket means the number of ouput classes of our classifiers, * means copy mechanism is
  used in our classifier.}
\label{tbl:phr:impl_phrasal}
\begin{center}
\begin{tabular}{l|cc}
\toprule
\hline
                                 & \multicolumn{2}{c}{{\bf Phrase Anchoring}} \\ \cline{2-3}
                                 & UCCA      & TOP                            \\ \hline
Root                             & 1         & 1                              \\ \hline
Node Label                       & N/A       & Intent(25), Slot(36)           \\ \hline
\multirow{2}{*}{Node Properties} & N/A       & N/A                            \\
                                 & N/A       & N/A                            \\ \hline
Edge Label                       & (15)      & N/A                            \\ \hline
Edge Properties                  & ``remote" & N/A                            \\ \hline
Connectivity                     & True      & True                           \\ \hline
Training Data                    & 6485      & 35741                          \\ \hline
Test Data                        & 1131      & 9042                           \\ \hline \bottomrule
\end{tabular}
\end{center}
\label{tbl:phr:impl_phrasal}
\end{table}

\subsection{Summary of Implementation}
\label{ssec:impl_summary}

We summarize our implementation for lexical-anchoring
in~\autoref{tbl:lex:impl_lex}, and phrasal-anchoring
in~\autoref{tbl:phr:impl_phrasal}. As we mentioned in the previous
sections, we use latent-alignment graph-based parsing for
lexical-anchoring rerepsentations~(DM, PSD, AMR), and use CKY-based
constituent parsing phrasal anchoring rerepsentations~(UCCA, TOP). This section
gives information about various decision for our models.

\subsubsection{Root}
\label{sssec:lex:root}

The first row \tquoted{Root} shows the numbers of root nodes in the graph.
We can see that for PSD, 11.56\% of graphs with more than 1 top
nodes. In our system, we predict one and only one root node with a
N~(N is size of already identified nodes) way classifier, and then fix
this with a postprocessing strategy. When our model predicts one node
as the root node, and if we find additional coordination nodes with
it, we add the coordination node also as the top node.

\subsubsection{Node}
\label{sssec:lex:node}

Except for UCCA, all other four representations have labeled
nodes, the row \tquoted{Node Label} shows the templates of a node label. For
DM and PSD, the node label is usually the lemma of its underlying
token. But the lemma is neither the same as one in the given companion
data nor the predicted by Stanford Lemma Annotators. One common
challenge for predicting the node labels is the open label set
problem. Usually, the lemma is one of the morphology derivations of
the original word. But the derivation rule is not easy to create
manually. In our experiment, we found that handcrafted rules for lemma
prediction only works worse than classification with copy mechanism,
except for DM.

For AMR, there are other components in the node labels beyond the
lemma. Especially, the node label for AMR also contains more than 143
fine-grained named entity types. In addition to the node label, the
properties of the label also need to be predicted. Among them, node
properties of DM are from the sense and argument handlers in Semantic
Interface~\citep[SEM-I,][]{Fli:Lon:Dyv:05}), while for PSD, senses are
constrained the senses in the predefined the vallex lexicon.

\subsubsection{Edge}
\label{sssec:lex:edge}

Edge predication is another challenge in our task because of its large
label set (from 45 to 94) as shown in row \tquoted{Edge Label,}~the
round bracket means the number of output classes of our
classifiers. For lexical-anchoring representation, edges are usually
connected between two tokens, while phrasal-anchoring needs extra
effort to figure out the corresponding span with that node. For
example, in UCCA parsing, to predict edge labels, we first predicted
the node spans, and then predict the node label based on each span,
and finally we transform back the node label into edge label.

\subsubsection{Connectivity}
\label{sssec:lex:connectivity}

Beside the local label classification for nodes and edges, there are
other global structure constraints for all five representations: All the nodes and
edges should eventually form a connected graph. For lexical anchoring,
we use MSCG algorithm to find the maximum connected graph greedily;
For phrasal anchoring, we use dynamic programming to decoding the
constituent tree then deterministically transforming back to a
connected UCCA Graph. We ignored all the discontinuous span and remote
edges in UCCA.

\subsection{Model Setup}
\label{ssec:exp_setup}

For lexical-anchoring model setup, our network mainly consists of node
and edge prediction model. For AMR, DM, and PSD, they all use one
layer Bidirectional LSTM for input sentence encoder, and two layers
Bidirectional LSTM for head or dependent node encoder in the
biaffine classifier. For every sentence encoder, it takes a sequence
of word embedding as input (We use 300 dimension Glove here), and then
their output will pass a softmax layer to predicting output
distribution. For the latent AMR model, to model the posterior
alignment, we use another BiLSTM for node sequence encoding. For
phrasal-anchoring model setup, we follow the original model set up in
\citet{kitaev2018constituency}, and we use 8-layers 8-headers
transformer with position encoding to encode the input sentence.

For all sentence encoders, we also use the character-level CNN model
as character-level embedding without any pretrained deep
contextualized embedding model. Equipping our model with Bert or
multitask learning is promising to get further improvement. We leave
this as our future work.

Our models are trained with Adam~\cite{kingma2014adam}, using a batch
size 64 for a graph-based model, and 250 for CKY-based
model. Hyper-parameters were tuned on the development set, based on
labeled F1 between two graphs. We exploit early-stopping to avoid
overfitting.

\subsection{Results}
\label{ssec:results}
At the time of official evaluation in MRP'2019 shared task, we
submitted the unified lexical anchoring parser for DM, PSD and AMR,
and then we submitted another phrasal-anchoring model for UCCA parsing
during postevaluation stage, and we leave EDS parsing as future
work. The following sections are the official results and error
breakdowns for lexical anchoring and phrasal anchoring, respectively. For the results of TOP Parsing, we use a seperate table
for it.

\subsubsection{Official Results on Lexical Anchoring}
\label{sssec:lex-phr:lex-results}
Table \ref{tbl:results_rank} shows the official results for our
lexical-anchoring models on AMR, DM, PSD.  By using our latent
alignment based AMR parser, our system ranked top 1 in the AMR
subtask, and outperformed the top 5 models in large margin. However,
the official results on DM and PSD shows that there is still around
2.5 points performance gap between our model and the top 1 model. Our
submitted parser ranked 6th on PSD and 7th on DM. Extensive error
analysis shows that our parser perform worse on the root and edge
prediction. Please refer to more details
in~\autoref{sssec:lex-phr:error-lex}.

\begin{table}[!tbp]
\caption{\label{tbl:results_rank} Official results overview on unified MRP metric, we selected the performance from top 1/3/5 system(s) for comparison.}
\begin{center}
\begin{tabular}{lll}
\toprule
MR     & Ours~(P/R/F1) & Top 1/3/5~(F1)  \\ \hline
AMR(1) & {\bf 75/71/73.38}   & {\bf 73.38/71.97/71.72} \\
PSD(6) & 89/89/{\bf 88.75}   & 90.76/89.91/{\bf 88.77} \\
DM(7)  & 93/92/92.14   & 94.76/94.32/93.74 \\ \hline
%UCCA   & 76/68/71.65   & 81.67/77.80/73.22 \\
%EDS    & N/A           & 94.47/90.75/89.10 \\ \bottomrule
\end{tabular}
\end{center}
\end{table}

\subsubsection{Official Results on Phrasal Anchoring~(UCCA)}
\label{sssec:lex-phr:ucca-results}
Table \ref{tbl:ucca_results_rank} shows that our span-based CKY model
for UCCA can achieve 74.00 F1 score on official test set, and ranked
5th. When adding ELMo~\cite{peters2018deep} into our model, it can
further improve almost 3 points on it. However, it still performs 4
points worse than the top 1 model from~\cite{Che:Dou:Xu:19}, which
realize stack LSTM with more advanced contextualized word embeddings
BERT~\cite{devlin2019bert}. We also conduct extensive error analysis
in~\autoref{sssec:lex-phr:error-phr}.

\begin{table}[!tbp]
\caption{\label{tbl:ucca_results_rank} Official results overview on
  unified MRP metric, we selected the performance from top 1/3/5
  system(s) for comparison. It shows our UCCA model for postevluation
  can rank 5th.}
  \small
\begin{center}
\begin{tabular}{lll}
\toprule
MR     & Ours~(P/R/F1) & Top 1/3/5~(F1)  \\ \hline
UCCA(5)   & 80.83/73.42/\textbf{76.94}   & 81.67/77.80/73.22 \\
\bottomrule
% EDS    & N/A                 & 94.47/90.75/89.10 \\ \bottomrule
\end{tabular}
\end{center}
\end{table}


\subsubsection{Results on Phrasal Anchoring~(TOP)}
\label{sssec:lex-phr:top-results}
\autoref{tbl:top_results_rank} shows the results on TOP parsing. We
use our CKY model skeleton but with different underlying
contextualized span representation from ELMo, Roberta, and
SpanBERT. Below the line are the baseline models for TOP parsing. Our
plain model with glove-based transformer model can outperform all the
previous baseline models. Contualized presentation from pretrained
language model can boost the performance further.

\begin{table}[!tbp]
\caption{\label{tbl:top_results_rank} Results on TOP parsing.}
\begin{center}
\begin{tabular}{cccccc}
  \toprule
  Model           & Exact Match & P     & R     & F1    \\ \hline
  Ours            & 80.85       & 92.77 & 91.05 & 91.9  \\
  Ours + ELMo     & 83.04       & 93.57 & 92.37 & 92.97 \\
  Ours + Roberta  & 85.89       & 96.07 & 93.08 & 94.92 \\
  Ours + SpanBERT & 85.82       & 95.55 & 94.44 & 94.99 \\ \hline
  RNNG                & 78.51       & 90.62 & 89.84 & 90.23 \\
  Seq2Seq-CNN         & 75.87       & 89.25 & 87.88 & 88.56 \\
  Seq2Seq-LSTM        & 75.31       & 88.35 & 87.03 & 87.69 \\
  Seq2Seq-Transformer & 72.2        & 87.09 & 86.11 & 86.6  \\
  \bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Error Breakdown}
\label{ssec:error_breakdown}
Tables~\ref{tbl:results_amr}, \ref{tbl:results_dm},
\ref{tbl:results_psd} and \ref{tbl:results_ucca} shows the detailed
error breakdown of AMR, DM, PSD and UCCA, respectively. Each column in
the table shows the F1 score of each subcomponent in a graph: top
nodes, node lables, node properties, node anchors, edge labels, and
overall F1 score. No anchors for AMR, and no node label and propertis
for UCCA. We show the results of MRP metric on two datasets. ``all"
denotes all the examples for that specific MR, while lpps are a set of
100 sentences from \texttt{The Little Prince}, and annotated in all five
meaning representations. To better understand the performance, we also
reported the official results from two baseline models
TUPA~\cite{Her:Arv:19} and ERG~\cite{Oep:Fli:19}.

\begin{table}[!tbp]
\caption{\label{tbl:results_amr} Our parser on AMR ranked 1st. This table shows the error breakdown when comparing to the baseline TUPA model and top 2~\cite{Che:Dou:Xu:19} in official results.}
\begin{center}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lcccccc}
\toprule
                          & data & tops              & labels & prop  & edges & all   \\ \hline
\multirow{2}{*}{ \parbox{1cm}{TUPA
single} }                 & all  & 63.95             & 57.20  & 22.31 & 36.41 & 44.73 \\
                          & lpps & 71.96             & 55.52  & 26.42 & 36.38 & 47.04 \\ \hline
\multirow{2}{*}{ \parbox{1cm}{TUPA
multi} }                  & all  & 61.30             & 39.80  & 27.70 & 27.35 & 33.75 \\
                          & lpps & 72.63             & 50.11  & 20.25 & 33.12 & 43.38 \\ \hline
\multirow{2}{*}{ Ours(1)} & all  & \underline{65.92} & 82.86  & {\bf 77.26} & 63.57 & {\bf 73.38} \\
                          & lpps & \underline{72.00} & 78.71  & 58.93 & 63.96 & 71.11 \\ \hline
\multirow{2}{*}{ Top 2}  & all  & 78.15             & 82.51  & 71.33 & 63.21 & 72.94 \\
                          & lpps & 83.00             & 76.24  & 51.79 & 60.43 & 69.03 \\ \bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[!tbp]
\caption{\label{tbl:results_dm} Our parser on DM ranked 7th. This table shows the error breakdown when comparing to the model ranked Top 1~\citep{Li:Zha:Zha:19} in official results}
\begin{center}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{cccccccc}
  \toprule
                            & data & tops              & labels & prop  & anchors & edges & all   \\ \hline
  \multirow{2}{*}{ ERG }    & all  & 91.83             & 98.22  & 95.25 & 98.82   & 90.76 & 95.65 \\
                            & lpps & 95.00             & 97.32  & 97.75 & 99.46   & 92.71 & 97.03 \\
  \multirow{2}{*}{Top 1}    & all  & 93.23             & 94.14  & 94.83 & 98.40   & 91.55 & 94.76 \\
                            & lpps & 96.48             & 91.85  & 94.36 & 99.04   & 93.28 & 94.64 \\
  \multirow{2}{*}{ Ours(7)} & all  & \underline{70.95} & 93.96  & 92.13 & 97.25   & 86.45 & 92.14 \\
                            & lpps & \underline{84.00} & 90.55  & 91.91 & 97.96   & 87.24 & 91.82 \\ \bottomrule
\end{tabular}
\end{center}
\end{table}



\begin{table}[!tbp]
\caption{\label{tbl:results_psd} Our parser on PSD ranked 6th. This table shows the error breakdown when comparing to the model ranked top 1~\cite{Don:Fow:Gro:19} in official results.}
\begin{center}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{cccccccc}
\toprule
                          & data & tops              & labels & prop  & anchors & edges & all   \\ \hline
%\multirow{2}{*}{ TUPA
%single }                 & all  & 53.07             & 64.58  & 43.30 & 79.50   & 32.96 & 52.97 \\
%                         & lpps & 59.90             & 66.44  & 42.46 & 81.45   & 38.11 & 55.73 \\
%\multirow{2}{*}{ TUPA
%multi }                  & all  & 53.07             & 64.58  & 43.30 & 79.50   & 32.96 & 52.97 \\
%                         & lpps & 59.90             & 66.44  & 42.46 & 81.45   & 38.11 & 55.73 \\
\multirow{2}{*}{Top 1}   & all  & 93.45             & 94.68  & 91.78 & 98.35   & 77.79 & 90.76 \\
                          & lpps & 93.33             & 91.73  & 84.37 & 98.40   & 77.63 & 88.34 \\
\multirow{2}{*}{ Ours(6)} & all  & \underline{82.01} & 94.18  & 91.28 & 96.94   & 72.40 & 88.75 \\
                          & lpps & \underline{85.85} & 90.48  & 82.63 & 95.97   & 73.60 & 85.83 \\ \bottomrule
\end{tabular}
\end{center}
\end{table}

\subsubsection{Error Analysis on Lexical Anchoring}
\label{sssec:lex-phr:error-lex}
As shown in~\autoref{tbl:results_amr}, our AMR parser is good at
predicting node properties and consistently perform better than other
models in all subcomponent, except for top prediction. Node properties
in AMR are usually named entities, negation, and some other quantity
entities. In our system, we recategorize the graph fragements into a
single node, which helps for both alignments and structured inference
for those special graph fragments. We see that all our 3 models
perform almost as good as the top 1 model of each subtask on node
label prediction, but they perform worse on top and edge
prediction. It indicates that our biaffine relation classifier are
main bottleneck to improve. Moreover, we found the performance gap
between node labels and node anchors are almost consistent, it
indicates that improving our model on predicting \texttt{NULL} nodes may
further improve node label prediction as well.  Moreover, we believe
that multitask learning and pretrained deep models such as
BERT~\citep{devlin2018bert} may also boost the performance of our paser
in future.

\subsubsection{Error Analysis on Phrasal Anchoring}
\label{sssec:lex-phr:error-phr}
Our UCCA parser in postevaluation ranked 5th according to the
original official evaluation results. According
to~\autoref{tbl:results_ucca}, our model with ELMo works slightly
better than the top 1 model on anchors prediction. It means our model
is good at predicting the nodes in UCCA and we belive that it is also
helpful for prediction phrasal anchoring nodes in EDS.

\begin{table}[!tbp]
  \caption{\label{tbl:results_ucca} The error breakdown of our UCCA
    parser. * denotes the ranking of postevaluation results.}
\begin{center}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{ccccccccc}
\toprule
                              & data & tops  & anchors & edge  & attr  & all   \\ \hline
\multirow{2}{*}{ TUPA
single }                      & all  & 78.73 & 69.17   & 16.96 & 15.18 & 27.56 \\
                              & lpps & 86.03 & 76.26   & 28.32 & 24.00 & 40.06 \\
\multirow{2}{*}{ TUPA
multi }                       & all  & 84.92 & 65.74   & 12.99 &  9.07 & 23.65 \\
                              & lpps & 88.89 & 77.76   & 26.45 & 18.32 & 41.04 \\
\multirow{2}{*}{\citet{Che:Dou:Xu:19}}       & all  & 1.00  & 95.36   & 72.66 & 61.98 & 81.67 \\
                              & lpps & 1.00  & 96.99   & 73.08 & 48.37 & 82.61 \\ \hline
\multirow{2}{*}{ Ours(*5)}    & all  & 98.85 & 94.92   & 60.17 & 0.00  & {\bf 74.00} \\
                              & lpps & 96.00 & 96.75   & 60.20 & 0.00  & 75.17 \\
\multirow{2}{*}{ Ours + ELMo} & all  & 99.38 & 95.70   & 64.88 & 0.00  & {\bf 76.94} \\
                              & lpps & 98.00 & 96.84   & 66.63 & 0.00  & 78.77 \\ \bottomrule
\end{tabular}
\end{center}
\end{table}

However, when predicting the edge and edge attributes, our model
performs 7-8 points worse than the top 1 model. In UCCA, an edge label
means the relation between a parent nodes and its children. In our
UCCA transformation, we assign edge label as the node label of its
child and then predict with only child span encoding. Thus it actually
misses important information from the parent node. Hence, in future,
more improvement can be done to use both child and parent span
encoding for label prediction, or even using another span-based
biaffine classifier for edge prediction, or remote edge recovering.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../dissertation-main.ltx"
%%% End:
