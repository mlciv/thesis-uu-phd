\subsection{Encoding Dialogue}
\label{ssec:dialog_rep}

%\vs{See comment about subsection titles above}

%\vs{Start each subsection with a description of what the underlying problem is. Here, the problem is: To build any c%lassifier over a dialogue, we need to encode them into a vector form. Dialogues are sequences of utterences, which t% hemselves are sequences of tokens. This hierarchical structure has been widely studied in past work and the typical% approach is to use a hierarchical recurrent neural network [cite]...}

%Through extensive experiments in
%\autoref{sec:experiments}, we find out the best combination of them for
%each task, and show what helps and what doesn't in each task with
%abalation studies in~\autoref{sec:analysis}.

Since both our tasks are classification tasks over a dialogue
history, our goal is to convert the sequence of utterences into a
single vector that serves as input to the final classifier.

We will use a hierarchical recurrent encoder~\cite[and
others]{li2015hierarchical,sordoni2015hierarchical,serban2016building}
to encode dialogues, specifically a hierarchical gated recurrent
unit (\HGRU) with an utterance and a dialogue encoder. We use a
bidirectional GRU over word embeddings to encode utterances. As is
standard, we represent an utterance $u_i$ by concatenating the final
forward and reverse hidden states. We will refer to this utterance
vector as $\bm{v}_i$. Also, we will use the hidden states of
each word as inputs to the attention components in~\autoref{ssec:word_att}. We will refer to such contextual word
encoding of the $j^{th}$ word as $\bm{v}_{ij}$. The dialogue encoder
is a unidirectional GRU that operates on a concatenation of
utterance vectors $\bm{v}_i$ and a trainable vector representing the
speaker $s_i$.\footnote{For the dialogue encoder, we use a
  unidirectional GRU because the dialogue is incomplete. For words,
  since the utterances are completed, we can use a BiGRU.} The final
state of the GRU aggregates the entire dialogue history into a
vector $\bm{H}_n$.

% One way to encode a whole dialogue is concatenating all the dialogue
% utterances into a single long sentence and then encode it with a
% sequence encoder such as LSTM. However, it may lose important
% structure information of the dialogue after being flattened. Another
% widely used method is using hierarchical recurrent
% encoder~\cite{li2015hierarchical,
%   yang2016hierarchical,tang2015document, sordoni2015hierarchical,
%   serban2016building, tran2017hierarchical,
%   serban2017multiresolution}. In this paper, we adopt a hierarchical
%   GRU~(HGRU) as our model skeleton.

% An alternative to the \HGRU is to flatten the history into a
% sequence of utterences and use a single biGRU to encode this
% flattened history. We will refer to this strategy for encoding the
% dialogue history as \CON and denote the bidirectional encoding of
% the history as $\bm{C}$ and the hidden states of this BiGRU as
% $\bm{c}_1, \bm{c}_2, \cdots$.

The \HGRU skeleton can be optionally augmented with the
word and dialogue attention described next. All the models we will study are two-layer MLPs over the
vector $\bm{H}_n$ that use a ReLU hidden layer and a softmax layer
for the outputs.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../dissertation-main.ltx"
%%% End:
