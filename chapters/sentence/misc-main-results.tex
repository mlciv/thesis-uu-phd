\subsection{Results}
\label{ssec:snt:results}

Our goal is to discover the best client and therapist models for the
two tasks. We first summarize the best configuration and the
corresponding performance of our best models for both categorzing and
forecasting MISC codes in ~\autoref{tbl:main_rst} with precision,
recall and $F_{1}$ for each codes. Then, we also show the performance
of these models against various baselines.
\begin{table}[h]
\begin{center}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|ccc|ccc}
\toprule
\hline
\multirow{2}{*}{{\bf Label}} & \multicolumn{3}{c}{{\bf Categorizing}} & \multicolumn{3}{c}{{\bf Forecasting}}  \\ \cline{2-4} \cline{5-7}
                             & P                                      & R    & $\text{F}_{1}$ & P    & R    & $\text{F}_{1}$ \\ \hline
\FN                          & 92.5                                   & 86.8 & 89.6    & 90.8 & 80.3 & 85.2    \\
\CHANGE                      & 34.8                                   & 44.7 & 39.1    & 18.9 & 28.6 & 22.7    \\
\SUSTAIN                     & 28.2                                   & 39.9 & 33.1    & 19.5 & 33.7 & 24.7    \\
\hline
\FA                          & 95.1                                   & 94.7 & 94.9    & 70.7 & 73.2 & 71.9    \\
\RES                         & 50.3                                   & 61.3 & 55.2    & 20.1 & 18.8 & 19.5    \\
\REC                         & 52.8                                   & 55.5 & 54.1    & 19.2 & 34.7 & 24.7    \\
\GI                          & 74.6                                   & 75.1 & 74.8    & 52.8 & 67.5 & 59.2    \\
\QUC                         & 80.6                                   & 70.4 & 75.1    & 36.2 & 24.3 & 29.1    \\
\QUO                         & 85.3                                   & 81.2 & 83.2    & 27.0 & 11.8 & 16.4    \\
\MIA                         & 61.8                                   & 52.4 & 56.7    & 27.0 & 10.6 & 15.2    \\
\MIN                         & 27.7                                   & 28.5 & 28.1    & 17.2 & 10.2 & 12.8    \\ \hline \bottomrule
\end{tabular}
\end{center}
\caption{Performance of our proposed
  models with respect to precision, recall and $\text{F}_{1}$ on categorizing and forecasting
  tasks for client and therapist codes}
\label{tbl:main_rst}
\end{table}

\Paragraph{Best Models} We identified the following best configurations using $\text{F}_{1}$
score on the development set:
\begin{enumerate}[nosep]
\item {\bf Categorization}: For client, the best model does not need any
  word or utterance attention. For the therapist, it uses \GMGRUH
  for word attention and \anchor for utterance attention. We refer
  to these models as $\mathcal{C}_C$ and $\mathcal{C}_T$ respectively
\item {\bf Forecasting}: For both client and  therapist, the best
  model uses no word attention, and uses \self utterance
  attention. We refer to these models as $\mathcal{F}_C$ and
  $\mathcal{F}_T$ respectively.
\end{enumerate}

\Paragraph{Results on Categorization}
Tables~\ref{tbl:main_rst_c_categorizing} and
\ref{tbl:main_rst_t_categorizing} show the performance of the
$\mathcal{C}_C$ and $\mathcal{C}_T$ models and the baselines.
%
For both therapist and client categorization, we compare the best
models against the same set of baselines. The majority baseline
illustrates the severity of the label imbalance
problem. \citet{xiao2016behavioral}, $\text{BiGRU}_{\text{generic}}$,
\citet{can2015dialog} and \citet{tanana2016comparison} are the
previous published baselines. The best results of previous published
baselines are underlined. The last row
$\Delta$ in each table lists the changes of our best model from
them. $\text{BiGRU}_{\text{ELMo}}$, $\text{CONCAT}^{C}$,
$\text{GMGRU}^{H}$ and $\text{BiDAF}^{H}$ are new baselines
we define below.

\begin{table}[!h]
\begin{center}{\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lcccc}
\toprule
Method                                         & macro                & \FN                    & \CHANGE              & \SUSTAIN             \\ \midrule
% BiGRU(Xiao reported)                         & 48.5                 & 87.0                   & 30.0                 & 28.6                 \\
Majority                                       & 30.6                 & {\bf \underline{91.7}} & 0.0                  & 0.0                  \\
\citet{xiao2016behavioral}                     & 50.0                 & 87.9                   & 32.8                 & \underline{29.3}     \\
$\text{BiGRU}_{\text{generic}}$                & \underline{50.2}     & 87.0                   & \underline{35.2}     & 28.4                 \\
$\text{BiGRU}_{\text{ELMo}}$                   & 52.9                 & 87.6                   & {\bf 39.2}           & 32.0                 \\
\midrule
\citet{can2015dialog}                          & 44.0                 & 91.0                   & 20.0                 & 21.0                 \\
\citet{tanana2016comparison}                   & 48.3                 & 89.0                   & 29.0                 & 27.0                 \\
% CONCAT                                       & 49.9                 & 89.7                   & 32.1                 & 27.8                 \\
$\text{CONCAT}^{C}$                            & 51.8                 & 86.5                   & 38.8                 & 30.2                 \\
%$\text{GMGRU}^{C}$                            & 44.8                 & 24.4                   & 26.0                 & 83.9                 \\
%$\text{BiDAF}^{C}$                            & 44.8                 & 24.4                   & 26.0                 & 83.9                 \\
$\text{GMGRU}^{H}$                             & 52.6                 & 89.5                   & 37.1                 & 31.1                 \\
$\text{BiDAF}^{H}$                             & 50.4                 & 87.6                   & 36.5                 & 27.1                 \\ \midrule
%\citet{huang2018modeling}                     & -                    & -                      & -                    & -                    \\ \midrule
  $\mathcal{C}_{C}$                            & {\bf 53.9}           & 89.6                   & 39.1                 & {\bf 33.1}           \\
$\Delta=\mathcal{C}_{C} - \text{\underline{score}}$ & {\footnotesize +3.5} & {\footnotesize -2.1}   & {\footnotesize +3.9} & {\footnotesize +3.8} \\
  \bottomrule
\end{tabular}}
\end{center}
\caption{\label{tbl:main_rst_c_categorizing} Main results on categorizing
  client codes, in terms of macro $\text{F}_{1}$, and $\text{F}_{1}$ for
  each client code. Our model $\mathcal{C}_C$ uses final dialogue
  vector $H_{n}$ and current utterance vector $v_{n}$ as input of MLP
  for final prediction. We found that predicting using
  $\text{MLP}(H_{n})+\text{MLP}(v_{n})$ performs better than just
  $\text{MLP}({H_{n}})$.}
\end{table}

\begin{table}[!htp]
\begin{center}{\small
\begin{tabular}{lccccccccc}
\toprule
Method                                         & macro                & \FA                  & \RES                 & \REC             & \GI              & \QUC             & \QUO             & \MIA             & \MIN             \\ \midrule
Majority                                       & 5.87                 & 47.0                 & 0.0                  & 0.0              & 0.0              & 0.0              & 0.0              & 0.0              & 0.0              \\
\citet{xiao2016behavioral}                     & 59.3                 & \underline{94.7}     & 50.2                 & 48.3             & 71.9             & 68.7             & 80.1             & 54.0             & 6.5              \\
$\text{BiGRU}_{\text{generic}}$                & \underline{60.2}     & 94.5                 & \underline{50.5}     & \underline{49.3} & 72.0             & 70.7             & 80.1             & \underline{54.0} & \underline{10.8} \\
$\text{BiGRU}_{\text{ELMo}}$                   & 62.6                 & 94.5                 & 51.6                 & 49.4             & 70.7             & 72.1             & 80.8             & 57.2             & 24.2             \\ \midrule
\citet{can2015dialog}                          & -                    & 94.0                 & 49.0                 & 45.0             & \underline{74.0} & \underline{72.0} & \underline{81.0} & -                & -                \\
\citet{tanana2016comparison}                   & -                    & 94.0                 & 48.0                 & 39.0             & 69.0             & 68.0             & 77.0             & -                & -                \\
$\text{CONCAT}^{C}$                            & 61.0                 & 94.5                 & 54.6                 & 34.3             & 73.3             & 73.6             & 81.4             & 54.6             & 22.0             \\
%$\text{GMGRU}^{C}$                            & 41.0                 & 92.4                 & 19.7                 & 39.1             & 65.1             & 39.4             & 65.0             & 7.4              & 0.0              \\
%$\text{BiDAF}^{C}$                            & 38.0                 & 90.7                 & 22.0                 & 19.4             & 62.5             & 41.9             & 61.2             & 6.5              & 0.0              \\
$\text{GMGRU}^{H}$                             & 64.9                 & 94.9                 & {\bf 56.0}           & 54.4             & {\bf 75.5}       & {\bf 75.7}       & {\bf 83.0}       & {\bf 58.2}       & 21.8             \\
$\text{BiDAF}^{H}$                             & 63.8                 & 94.7                 & 55.9                 & 49.7             & 75.4             & 73.8             & 80.7             & 56.2             & 24.0             \\ \midrule
%\citet{huang2018modeling}                     & -                    & -                    & -                    & -                & -                & -                & -                & -                &                  \\ \midrule
$\mathcal{C}_{T}$                              & {\bf 65.4}           & {\bf 95.0}           & 55.7                 & {\bf 54.9}       & 74.2             & 74.8             & 82.6             & 56.6             & {\bf 29.7}       \\
$\Delta=\mathcal{C}_{T} - \text{\underline{score}}$ & {\footnotesize +5.2} & {\footnotesize +0.3} & {\footnotesize +3.9} & {\footnotesize +3.8} & {\footnotesize +0.2} & {\footnotesize +2.8} & {\footnotesize +1.6} & {\footnotesize +2.6} & {\footnotesize +18.9}                                                                                                \\ \bottomrule
\end{tabular}}
\end{center}
\caption{\label{tbl:main_rst_t_categorizing} Main results on
  categorizing therapist codes, in terms of macro $\text{F}_{1}$, and
  $\text{F}_{1}$ for each therapist code. Models are the same as Table
  ~\ref{tbl:main_rst_c_categorizing}, but tuned for therapist
  codes. For the two grouped MISC set \MIA and \MIN, their results are
  not reported in the original work due to different setting.}
\end{table}



The first set of baselines (above the line) do not encode dialogue
history and use only the current utterance encoded with a BiGRU. The
work of \citet{xiao2016behavioral} falls in this category, and uses a
100-dimensional domain-specific embedding with weighted cross-entropy
loss. Previously, it was the best model in this class. We also
re-implemented this model to use either ELMo or Glove vectors with
focal loss.\footnote{Other related work in no context
  exists~\cite[e.g.,][]{perez2017predicting, gibson2017attention}, but
  they either do not outperform \cite{xiao2016behavioral} or use
  different data.}

The second set of baselines (below the line) are models that use
dialogue context.  Both \citet{can2015dialog} and
\citet{tanana2016comparison} use well-studied linguistic features and
then tagging the current utterance with both past and future utterance
with CRF and MEMM, respectively. To study the usefulness of the
hierarchical encoder, we implemented a model that uses a bidirectional
GRU over a long sequence of flattened utterance. We refer to this as
$\text{CONCAT}^{C}$. This model is representative of the work
of~\citet{huang2018modeling}, but was reimplemented to take advantage
of ELMo.



% (superscript
% $C$ means categorizing, for distinguishing models for forecasting
% task) is based on the CON skeleton, which use BIGRU to encode a
% concatenated dialogue history and use the output history encoding as
% extra features for classification. It contributes the main improvement
% in~\citet{huang2018modeling}~\footnote{ \citet{huang2018modeling}
%   studied categorizing only client codes on a different dataset. For
%   comparison, we reimplement the $\text{CONCAT}^{C}$ method with ELMo
%   in our paper, and also studied their proposed input add-ons
%   separately in \S\ref{ssec:abl_context_attention}, and we leave topic
%   model embedding as our future work.  }

For categorizing client codes, $\text{BiGRU}_{\text{ELMo}}$ is a
simple but robust baseline model. It outperforms the previous
best no-context model by more than 2 points on macro $\text{F}_{1}$. Using the
dialogue history, the more sophisticated model $\mathcal{C}_{C}$
further gets 1 point improvement. Especially important is its
improvement on the infrequent, yet crucial labels \CHANGE and
\SUSTAIN. It shows a drop in the $\text{F}_{1}$ on the \FN label, which is
essentially considered to be an unimportant, background class from
the point of view of assessing patient progress.
%
For therapist codes, as the highlighted numbers in Table
\ref{tbl:main_rst_t_categorizing} show, only incorporating GMGRU-based
word-level attention, $\text{GMGRU}^{H}$ has already outperformed many
baselines, our proposed model $\mathcal{F}_{T}$ which uses both
GMGRU-based word-level attention and anchor-based multi-head multihop
sentence-level attention can further achieve the best overall
performance. Also, note that our models outperform approaches
that take advantage of future utterances.

For both client and therapist codes, concatenating dialogue history
with $\text{CONCAT}^{C}$ always performs worse than the hierarchical
method and even the simpler $\text{BiGRU}_{\text{ELMo}}$.

\begin{table}[!h]
\centering
\begin{tabular}{lcccccc}
  \toprule
  \multirow{2}{*}{Method} & \multicolumn{2}{c}{Dev} & \multicolumn{4}{c}{Test}                                       \\ \cmidrule(lr){2-3} \cmidrule(lr){4-7}
                          & \CHANGE                 & \SUSTAIN   & macro      & \FN        & \CHANGE    & \SUSTAIN   \\ \midrule \midrule
%  BiGRU                  &                         &            &            &            &            &            \\
$\text{CONCAT}^{F}$       & 20.4                    & 30.2       & 43.6       & 84.4       & 23.0       & {\bf 23.5} \\
% CONCAT(no ELMo)         & 27.4                    & 18.0       & 36.6       & {\bf 87.7} & 18.5       & 18.2       \\
  HGRU                    & 19.9                    & 31.2       & {\bf 44.4} & 85.7       & {\bf 24.9} & 22.5       \\
  $\text{GMGRU}^{H}$      & 19.4                    & 30.5       & 44.3       & 87.1       & 23.3       & 22.4       \\ \midrule
%  $\text{GMGRU}^{C}$     & 8.7                     & 28.5       & 40.4       & 87.0       & 11.7       & 22.6       \\
  $\mathcal{F}_{C}$       & {\bf 21.1}              & {\bf 31.3} & 44.3       & 85.2       & 24.7       & 22.7       \\
\bottomrule
\end{tabular}
\caption{\label{tbl:main_rst_forecast:client} Main results on forecasting client codes, in terms of $\text{F}_{1}$ for \SUSTAIN, \CHANGE on dev set, and macro $\text{F}_{1}$, and $\text{F}_{1}$ for each client code on the test set.}
\end{table}

\begin{table}[!h]
\centering
\begin{tabular}{ccccccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{1}{c}{Recall} & \multicolumn{9}{c}{$\text{F}_{1}$}                                                                                             \\ \cmidrule(lr){2-2} \cmidrule(lr){3-11}
                        & R@3                        & macro      & \FA        & \RES       & \REC       & \GI        & \QUC       & \QUO       & \MIA       & \MIN       \\ \midrule \midrule
% BiGRU                 &                            &            &            &            &            &            &            &            &            &            \\
% CONCAT                & 72.0                       & 22.0       & 63.2       & 2.6        & 14.5       & 52.8       & 24.9       & 5.5        & 12.0       & 0.0        \\
$\text{CONCAT}^{F}$     & 72.5                       & 23.5       & 63.5       & 0.6        & 0.0        & 53.7       & 27.0       & 15.0       & 18.2       & 9.0        \\
% HGRU_generic          & 76.8                       & 24.0       & 71.0       & 2.7        & 20.5       & 58.8       & 27.5       & 12.9       & {\bf 15.2} & 1.6        \\
HGRU                    & 76.0                       & 28.6       & 71.4       & 12.7       & {\bf 24.9} & 58.3       & 28.8       & 5.9        & {\bf 17.4} & 9.7        \\
$\text{GMGRU}^{H}$      & 76.6                       & 26.6       & {\bf 72.6} & 10.2       & 20.6       & 58.8       & 27.4       & 6.0        & 8.9        & 7.9        \\ \midrule
%$\text{GMGRU}^{C}$     & 67.6                       & 19.6       & 62.0       & 11.8       & 19.7       & 51.6       & 10.0       & 1.9        & 0.1        & 0.0        \\ \midrule
$\mathcal{F}_{T}$       & {\bf 77.0}                 & {\bf 31.1} & 71.9       & {\bf 19.5} & 24.7       & {\bf 59.2} & {\bf 29.1} & {\bf 16.4} & 15.2       & {\bf 12.8} \\
\bottomrule
\end{tabular}
\caption{\label{tbl:main_rst_forecast:therapist} Main results on forecasting therapist codes, in terms of Recall@3, macro $\text{F}_{1}$, and $\text{F}_{1}$ for each label on test set}
\end{table}

\Paragraph{Results on Forecasting} Since the forecasting task is
new, there are no published baselines to compare against. Our
baseline systems essentially differ in their representation of
dialogue history. The model $\text{CONCAT}^{F}$ uses the same
architecture as the model $\text{CONCAT}^{C}$ from the categorizing
task. We also show comparisons to the simple \HGRU model and the
\GMGRUH model that uses a gated matchGRU for word
attention.\footnote{The forecasting task bears similarity to the
  next utterance selection task in dialogue state tracking
  work~\cite{DSTC7}. In preliminary experiments, we found that the
  Dual-Encoder approach used for that task consistently
  underperformed the other baselines described here.}

\autoref{tbl:main_rst_forecast:client} and
\autoref{tbl:main_rst_forecast:therapist} show our forecasting results
for client and therapist respectively. For client codes, we also
report the \CHANGE and \SUSTAIN performance on the development set
because of their importance.  For the therapist codes, we also report
the recall@3 to show the performance of a suggestion system that
displayed three labels instead of one.
%
The results show that even without an utterance, the dialogue history
conveys signal about the next MISC label. Indeed, the performance for
some labels is even better than some categorization baseline
systems. Surprisingly, word attention ($\text{GMGRU}^{H}$) in both
the~\autoref{tbl:main_rst_forecast:client} and
\autoref{tbl:main_rst_forecast:therapist} did not help in forecasting
setting, and a model with the \self utterance attention is
sufficient. For the therapist labels, if we always predicted the three
most frequent labels (\FA, \GI, and \RES), the recall@3 is only 67.7,
suggesting that our models are informative if used in this
suggestion-mode.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../thesis-main.ltx"
%%% End:
