\section{Models for MISC Prediction}
\label{sec:snt:devices}

Modeling the categorization and forcasting tasks defined in~\autoref{sec:snt:task} requires
addressing four questions:
\begin{itemize}
\item How do we encode a dialogue and its utterances?
\item Can we discover discriminative words in each utterance?
\item Can we discover which of the previous utterances are relevant?
\item How do we handle label imbalance in our data?
\end{itemize}
%
Many recent advances in neural networks can be seen as plug-and-play
components. To facilitate the comparative study of models, we will
describe components that address the above questions.  % Then, we
% will compare the models and their various ablations as a combination
% of these modules in~\autoref{sec:experiments} and \ref{sec:analysis}
% respectively.
%
In the rest of the chapter, we will use~\textbf{boldfaced} terms to
denote vectors and matrices and \module{small caps} to denote
component names.

% \vs{We spoke about the transfer learning point above.}
% \vs{Maybe these could be phrased as questions, and not in-para
% enum. That way, we could even add a reference to the subsection
% that discusses that question in parendissertation. The subsection titles
%  should use the words from the question. That way, it becomes easy
%  to read.}
%
%\begin{figure*}[t]
%\centering
%\includegraphics[width=0.3\textwidth]{hgru.png}\quad
%\includegraphics[width=0.3\textwidth]{gmlstm.png} \quad
%\includegraphics[width=0.3\textwidth]{multi-head.png} \quad
%\caption{\label{fig:models} (left) Hierarchical GRU, (middle) word-level Gated match-LSTM, (right) snt-level multi-head multi-hop self-attention. \vs{Do not use pngs if possible. Use pdf or eps}}
%\end{figure*}

\input{chapters/sentence/misc-dial-rep}

\input{chapters/sentence/misc-word-att}

\input{chapters/sentence/misc-snt-att}

\input{chapters/sentence/misc-pred-training}

\input{chapters/sentence/misc-focal-loss}


%\input{mtl_devices}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../dissertation-main.ltx"
%%% End:
