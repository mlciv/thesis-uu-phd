\section{Related Work}
\label{sec:sentential:related}
In this section, we review the related work on MISC code prediction,
different MISC code clustering strategies, and recent advances on
dialogue representation and attention mechanism.

\subsection{MISC Code Prediction}
\label{ssec:sentential:misc-related}
MEMM and CRF with handcrafted features are firstly proposed by
\citet{can2012case, can2015dialog} for MISC code prediction task
. Then \citet{tanana2015recursive} improved the model by incorporating
richer dependency relation features, which even outperformed a
proposed recursive neural network baseline model. Recently,
\cite{xiao2016behavioral} propose to use GRU with domain-specific word
embedding, and weighted cross-entropy loss to resolve the label
imbalance problem. In this paper, we study the solutions draw from
recent work in dialogue representation, memory attention, and
imbalanced classification. Besides that, another improvement exists,
such as topic models based domain adaptio~\cite{atkins2014scaling,
  huang2018modeling}, and prosodic features \cite{weber2002using} has
been proposed to improve the prediction tasks.

\input{chapters/sentence/clustering_strategies}

\subsection{Dialogue Representation and Hierarchical Encoder }
\label{ssec:sentential:dialogue-encoder}
End-to-End neural networks and attention mechanism has been widely
used in many natural language tasks, such as text classification,
question answering, dialogue system and so on.  RNN, CNN, and
Transformer has been quite effective for modeling sequence of
words~\cite{ kim14cnn,zhang2015character}. Combinations of them have
also been used to model the hierarchical structure of document, and
dialogue context. They first use CNN or LSTM to get a sentence vector,
and then a BiGRU to compose a sentence vectors to get a document
vector \cite{tang2015document, li2015hierarchical,
  yang2016hierarchical,sordoni2015hierarchical, serban2016building,
  serban2017multiresolution}. In our paper, we use hierarchical GRU as
skeletons. Other hierarhical combination may also help, we leave it
for future studies.

\subsection{Attention Mechanism}
\label{ssec:sentential:attention}
Attention mechanism was first proposed by \citet{bahdanau2014neural}
in machine translation, then various of extensions has been invented
and widely used in other tasks, especially for question answering and
dialogue
system\cite{matchlstm,bidaf,sukhbaatar15mnet,fei17gmnet,P18-1157}.
Attention on sentence-level representation also helps in many recent
works such as abstractive summarization~\cite{P18-1013}, dialogue
state tracking~\cite{zhou2018multi,zhou2016multi}, document
classification~\cite{yang2016hierarchical}.

Previous work on hierarchical attention\cite{yang2016hierarchical}
tries always to use both word-level and sentence -level attention in a
model, In this paper, we argue that, for different tasks, we need
different levels attentions. Always adding two-level attention may be
not necessary.  Recently, multi-heads multi-hops attention used in
Transformer \cite{NIPS2017_7181} became a more attractive attention
mechanism. All above advances in NLP inspired us to do systematic
analysis on whether or how they impact on our tasks.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../thesis-main.ltx"
%%% End:
