\section[Independent Factorization for Motivational
Interviewing]{Independent Factorization for Motivational \\Interviewing}
\label{sec:snt:task}
In this section, we will formally define the two NLP tasks
corresponding to the vision in~\autoref{sec:snt:background} using the
conversation in~\autoref{tbl:snt:misc-example} as a running
example. Suppose we have an ongoing MI session with utterances
$u_1, u_2,\cdots, u_n$: together, the dialogue history $H_n$.  Each
utterance $u_i$ is associated with its speaker $s_i$, either C
(client) or T (therapist). We will define two classification tasks
over a fixed dialogue history with $n$ elements ---
\emph{categorization}~\autoref{ssec:snt:task-categorization} and
\emph{forecasting}~\autoref{ssec:snt:task-forcasting}.  Then we make a
comparative analysis on the independent factorization for the two
tasks~\autoref{ssec:snt:comparative-tasks}.

%We will refer to the last utterance $u_n$ as the \emph{anchor} for the
%corresponding subtasks.  As the conversation progresses, the history
%will be updated with a sliding window.  Since the therapist and client
%codes share no overlap, we will design separate models for the two
%speakers, giving us four settings in all.

\begin{table}[tp]
  \caption{\label{tbl:snt:misc-example} An example of ongoing therapy session}
  \begin{center}
    \setlength{\tabcolsep}{3pt}
    {
      \begin{tabular}{crll}
        \toprule
        $i$        & $s_{i}$ & $u_{i}$                             & $l_{i}$  \\ \hline
        1        & T:      & Have you used drugs recently?       & \QUC     \\
        2        & C:      & I stopped for a year, but relapsed. & \FN      \\
        3        & T:      & You will suffer if you keep using. & \MIN     \\
        4        & C:      & Sorry, I just want to quit.         & \CHANGE  \\
        $\cdots$ &         & $\cdots$                            & $\cdots$ \\ \bottomrule
      \end{tabular}
    }
  \end{center}
\end{table}

\subsection{Task 1: Categorization}
\label{ssec:snt:task-categorization}
The goal of this task is to provide real-time feedback to a
therapist during an ongoing MI session. In the running example, the
therapist's confrontational response in the third utterance is not
MI adherent (\MIN); an observer should flag it as such to bring the
therapist back on track. The client's response, however, shows an
inclination to change their behavior (\CHANGE). Alerting a therapist
(especially a novice) can help guide the conversation in a direction
that encourages it.

In previous post-hoc dialogue analysis setting, we have the following
definition for the categorization task: \emph{Given the dialogue
  history $H_n=\{u_{1},u_{2}, ..., u_{n}\}$ which includes the speaker
  information, output a sequence of MISC label
  $L_n=\{l_{1}, l_{2}, ..., l_{n}\}$ for each utterance $u_i$.}

In essence, we have the following real-time classification task:
\emph{Given the dialogue history $H_n$ which includes the speaker
  information, predict the MISC label $l_n$ for the last utterance
  $u_n$.}

The key difference from previous work in predicting MISC labels is
that we are restricting the input to the real-time setting. As a
result, models can only use the dialogue history to predict the label,
and in particular, we can not use models such as a conditional random
field or a bi-directional LSTM that need both past and future inputs.

% \jc{I added this for comparing to
% previous dialogue act classification}
% \vs{I removed the content because it repeats information from
% section 3}

% For the client codes, it is similar to the sentiment analysis but it
% towards more on behavior changes, e.g., the client may feel happy
% and confident that smoking may help to relax, but this is \misc{NEG}
% in our cases. For therapist codes, this task can be considered as
% dialogue intent classification but emphasize professional counseling
% skills and spirits, such as empathy, support, and evocation.


\subsection{Task 2: Forecasting}
\label{ssec:snt:task-forcasting}
A real-time therapy observer may be thought of as an expert
therapist who guides a session with suggestions to the therapist.
For example, after a client discloses their recent drug use
relapse, a novice therapist may respond in a confrontational manner
(which is not recommended, and hence coded \MIN). On the other
hand, a seasoned therapist may respond with a complex reflection
(\REC) such as \emph{``Sounds like you really wanted to give up and
  you're unhappy about the relapse.''}
Such an expert may also anticipate important cues
from the client.% , expressing their intention to change or sustain
% their behavior
% .
The MISC forecasting task is a previously unstudied problem in the
posthoc dialogue analysis.

The forecasting task seeks to mimic the intent of such a seasoned
therapist: \emph{Given a dialogue history $H_n$ and the next speaker's
  identity $s_{n+1}$, predict the MISC code $l_{n+1}$ of the yet
  unknown next utterance $u_{n+1}$.}
% We
% argue that it is possible to learn to anticipate information about
% future utterances; this forms the basis of chatbot
% development.

We argue that forecasting the type of the next utterance, rather than
selecting or generating its text as has been the focus of several
recent lines of work~\citep[\eg,][]{schatzmann2005quantitative,ubuntu,DSTC7},
allows the human in the loop~(the therapist) the freedom to
creatively participate in the conversation within the parameters
defined by the seasoned observer, and perhaps even rejecting
suggestions. Such an observer could be especially helpful for
training therapists~\citep{imel2017technology}.
%
The forecasting task is also related to recent work on detecting
antisocial comments in online
conversations~\citep{zhang2018conversations} whose goal is to
provide an early warning for such events.

\subsection[Independent Factorization for Categorization and
Forcasting Task]{Independent Factorization for Categorization \\and
  Forcasting Task}
\label{ssec:snt:comparative-tasks}

Take the dialogue segment in~\autoref{tbl:snt:misc-example} as a
running example, we compare the categorization and forecasting task
for each turn, with respect to the independent factorization for the
dialogue history and predicting target. Both categorization and
forecasting tasks are taking a sequence of dialogue utterances as
input, and then predict a sequence of MISC labels as output. They
share similar sequence labeling structures and seem to share the
same independent factorization. However, they have different
predicting goals when considering each dialogue turn. In the
following, we first introduce how we decompose the sequence of output
MISC labels according to the speaker, and then we present the
sliding window to decompose the input dialogue
history. \autoref{tbl:snt:task-comparasion} shows the clear
differences when we choose dialogue history window size as 3.

\subsubsection{Output Decomposition}
\label{sssec:snt:output-decomposition}
The outputs of both tasks are a sequence of MISC labels. Because each
MISC label is naturally associated with a corresponding utterance,
representing different functions as shown in
\autoref{tbl:bg:misc}. Hence for independent factorization, it is
natural to decompose the whole sequence prediction into a set of
continuous single MISC label prediction tasks as defined
previously. In such a way, each MISC prediction in the categorization
or forecasting task is independent of the MISC prediction in the
previous dialogue turn. However, they have different goals for each
dialogue turn. When the dialogue goes to turn 3, both the
categorization and forecasting task will observe the current dialog
history window as input $X=H_{n}$. However, the key difference is as
follows: the categorization task is to predict the MISC code $l_{n}$
for the last seen utterance $u_{n}$, while the forecasting task is to
guess the future MISC code $l_{n+1}$ for the unseen utterance
$u_{n+1}$. To model this difference on output decomposition, we
introduce four components for dialogue and sentence representation
learning and search for the best model for each
task~\autoref{sec:snt:devices}. Furthermore, the MISC codes for the
clients and therapist represent different meanings and functions for
motivational skills. For example, the client MISC codes~( \SUSTAIN,
\CHANGE, and \FN) discuss the client's intention to change or sustain
problematic behavior or be neutral. While the therapist codes
represent the actions the therapist takes for a motivational
interview~(e.g., giving information~\GI, simple reflection~\RES). To
simplify the modeling, we also decompose the output according to the
speakers. For example, for the categorization task, we build two
models $C_{c}$ and $C_{t}$ for client and therapist codes
separately. Similar for the forecasting task, we study the $F_{c}$ and
$F_{t}$ for client and therapist. After such output decomposition,
every model only needs to classify on a unified set of MISC labels,
representing coherent goals for the clients or the therapist.

\subsubsection{Input Decomposition and Alignments Discovery}
\label{sssec:snt:input-decomposition}
For the input decomposition in previous lexical-anchoring and
phrasal-anchoring, the corresponding output label can be directly
derived from each input segment. For example, in lexical anchoring,
each concept or subgraph in an AMR graph is aligned to each token
or special entity. In phrasal anchoring, a non-terminal intent and
slot label in TOP is aligned to a phrase in the input sentence. In
MISC code prediction, the MISC code is assigned to a
utterance. However, the MISC code can not be derived by only a single token
, phrase or the aligned utterance. According to the MISC annotation
guidedline, the identification of relevant words or utterances may
depend on the whole dialogue history. For example, reflection related
MISC~(\RES, \REC) may need to discover which utterance in the dialogue
history the therapist are reflecting to. Hence, more than the previous
hard-aligned lexical or phrasal-anchoring, it requires to discovering
the relevant details in the meaning of the sentence \textbf{within the
  context of dialogue history}. In ~\autoref{sec:snt:devices}, we
proposed a hierarchical dialogue encoder to model the nest structures
of dialogue, and we also offered word-level and utterance-level
attention to help discover the relevant parts in it.

Due to the realtime setting, the input dialogue is naturally
decomposed by time steps, and forms a sequence of incremental dialogue
history $\{u_1\}$, $\{u_1,u_2\}$, and $\{u_1,u_2,u_3\}$.  When the dialogue goes to the turn 4, the dialog history will
slide to the next window of size 3, as $H_{n}=\{u_2,u_3,u_4\}$. The
$u_{1}$ will be truncated due the sliding window. We limit the
dialogue window because the whole therapy dialogue session may last
for 500 utterances, where current neural models such as RNN and
transformer can not handle the long context well.  In this dissertation
study~\autoref{ssec:snt:abl_context_attention}, we compare the window
size as 8 and 16 for our models. We leave the extremely long dialogue
context encoding as the future work.

\begin{table}[!h]
\caption{\label{tbl:snt:task-comparasion} The differences between the
  categorization task and the forcasting task, when choosing a window
  size as 3 to factorize the dialog sequential flow}
\begin{center}{
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c}
\toprule
\hline
\multirow{2}{*}{Turn} & \multirow{2}{*}{$X=H_{n}$} & Categorization & Forcasting  \\
                      &                            & $Y=l_{n}$      & $Y=l_{n+1}$ \\ \hline
 1                    & $\{u_1\}$                  & {\QUC}         & {\FN}       \\
 2                    & $\{u_1,u_2\}$              & {\FN}          & {\MIN}      \\
 3                    & $\{u_1,u_2,u_3\}$          & {\MIN}         & {\CHANGE}   \\
 4                    & $\{u_2,u_3,u_4\}$          & {\CHANGE}      & {\RES}      \\ \hline
  \bottomrule
\end{tabular}}
\end{center}
\end{table}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../dissertation-main.ltx"
%%% End:
