\section{Future Work}
\label{sec:conclusions:future}

\subsection{For Other Factorization}
\label{ssec:future:other-factorization}

This dissertation mainly focuses on independent factorization, which ignores
the interdependence between output parts. Our experiments show that
the contextualized representation can capture the interdependence
within the input parts; thus, they can offer discriminative features
to predict each output part independently. However, we also can extend
our work on other factorizations such as \kw{auto-regressive
  factorization}, or arbitrary high order factorizations. In those
cases, the output parts will either depend on the previously predicted
output or have other high order dependecies. Take the auto-regressive
factorization as an example, we consider a more broad-coverage
construction of output \OUT~as sequential decisions as shown
in~\autoref{eq:auto-factor}.
\begin{equation}
  \label{eq:auto-factor}
E(x, y)=\sum_{j=0}^{M}E(y_{j}|x,y_{<j})
\end{equation}
In autoregressive factorization, as shown in
~\autoref{fig:autoreg-example}, for every step, a new decision $y_{j}$
will depend on the input $x$ and previous decisions $y_{<j}$. In this
case, the main challenge of the model is to learn the representation
of $x$ and previous sequential decisions $y_{<j}$, then make a
decision $y_{j}$ based on them.  Especially inspired by distributed
representation of the natural language, we also study the distributed
representation of the output structures $y_{<j}$, and leverage them to
guide constrained structured prediction.

\begin{figure}[h]
\centering
\includegraphics[width=0.92\textwidth]{seq2graph.pdf}
\caption{\label{fig:autoreg-example}The autoregressive factorization
  of AMR Parsing in different decoding time step}
\end{figure}

\subsection{Apply to Other Symbolic Representations}
\label{ssec:future:application}
As shown in~\autoref{chap:lexical-phrasal}, the above structural
inductive bias in lexical, phrasal, and sentential anchoring can be
easily extended to other linguistic structured prediction tasks, such
as coreference the resolution, semantic role labeling, where the main
task structures has been studied in our broad-coverage meaning
representation parsing.

Let us take the \kw{coreference resolution} tasks as an example, we
show how to apply the independent factorization on a new
task. Coreference resolution is the task of clustering mentions in
text that refer to the same underlying real-world entities or
events. As shown in the left bottom of \autoref{fig:intro:dog-amr},
``dog" and ``it" are pointed to the same dog.

\Paragraph{Output Decomposition}
A classic problem formula for coreference resolution task is to define
a set of antecedent assignments $y_{i}$ for each of span $x_{i}$ in
the given document. The set of possible assignments for each
$y_{i} \in {\Phi, 1,...i-1}$. $\Phi$ means dummy antecent or the span
$i$ is not a mention, and every $y_{i}$ can be assigned with the
preceding spans.  Hence, then there are three factors for the
pair-wise coreference core.
\begin{inparaenum}[(1)]
\item predict $I_{m}(i)$, whether span $i$ is a mention.
\item predict $I_{m}(j)$, whether span $j$ is a mention.
\item search for $y_{i}$, based on the previous mention predictions
  $I_{m}(i)$ and $I_{m}(j)$.
\end{inparaenum}

One problem with the above factorization is that the third factor
$y_{i}$ may have multiple values, which are antecedent for multiple
preceding spans. Furthermore, it is not easy to do $N$ ways
classification directly, because for each $y_{i}$ the output candidate
labels are different, and the features only from span $i$ may not
enough to produce the $y_{i}$.

Instead, a better independence factorization can model this into
a two-stage pipeline model as shown in AMR parsing: Stage 1, predicting
the possible mentions via local classifier for each local factor
$I_{m}(i)$. Stage 2, predicting the edges labels between all pairs as
local binary classification $I_{a}(i,j)$, which means whether span $i$
and $j$ are in the same cluster without caring about the preceding or
not. In this output factorization, we can model two part of
contextualized representation from span $i$ and span $j$. Such a
pair-wise factorization will offer more discriminative features and
simplifer all the classification into binary classification.  We still
can use a bi-affine classifier to model the pair-wise binary
classification. Furthermore, logic constraints can be added here to
enforce the consistency between local decisions.

\Paragraph{Input Decomposition}
According to the above analysis on output decomposition, we need to
decompose the inputs into candidate spans. Assuming there are $N$
words in the input document, then there will be~$\frac{T(T+1)}{2}$
possible spans. However, we don't need to consider the spans cross the
sentence boundaries, and we only mainly consider the pronouns, nouns
, and other entity-related or event-related spans. Hence, such
inductive biases about extracting potential spans will also simplify
the input decomposition. At the same time, the pruning of the spans
will reduce the computation for the second stage on binary edge label
prediction.

Hence, our proposed methods for two-stage graph-based parsing can be
used in coreference resolution with task-specific output
decompositions and input decompositions. The contextualized
representation for each span can also benefit from span representation
we studied in phrasal-anchoring parsing~\autoref{ssec:phr:span}

For application-specific symbolic representation, besides the single
sentence representation
in~\cite[TOP,][]{gupta-etal-2018-semantic-parsing}, we also can extend
our structured prediction models into session-based conversational
representation such as session-based
TOP~\cite{aghajanyan2020conversational},
~\cite[TreeDST,][]{cheng2020conversational}, and
~\cite[Dataflow,][]{andreas2020task}.  Beyond conversational analysis,
in the future, I plan to exploit this structured analysis on symbolic
representation to offer rigorous document analysis, easier knowledge
organization, programmable reasoning, which are potentially helpful
for structured social analysis such as mental health, cyberbullying,
thus offering structural suggestions to guide human behavior.

\subsection{Future Work on Contextualized Representation}
\label{ssec:future:contextural-rep}
The strong power of
contextualized representation learning make out independent
factorization works still gold under our inductive biases. However,
there are still many challenges on contextualized representation
learning.

\subsubsection{Extreme Long Context}
\label{sssec:future:extrem-long-context}
First, we need to resolve the extrem long text encoding problem. Our
current models of psychotherapy dialogue and schema-guided dialog only
consider 8 to 16 utterances as the dialogue history window. However,
we have more than 500 utterance in a single therapy
session. Furthermore, a psychotherapy treatment may last for months
and years which involves multiple dialogue sessions. The extended
context problem also exists in other domains, such as scientific
document analysis and threaded conversations in social media.

\subsubsection{Contextualized Representation Beyond Text}
\label{sssec:future:beyond-text}
In this dissertation, we mainly utilize the contextualized text
representation to model the structured prediction. However, human
acquisition of information and communication with the world does not
occur based on pure language input. From a cognitive perspective,
processing language in isolation without information in other
modalities seems insufficient. Recently, multimodal contextualized
representation has also been widely used in the natural language
processing task. \citet{beinborn-etal-2018-multimodal} shows that
multimodal grouding of verbs play a crucial role for the compositional
power of language. Jointly considering both visual and textual models
has been widely used in many tasks, such as Multimodel Aspect-Based
Sentimental Analysis~(MABSA)~\cite{ju2021joint}, Named Entity
Recognization~\cite{zhang2021multi} and so on. Some tasks consider
both multimodal inputs and multimodal outputs, e.g., Question
Answering~\cite{singh-etal-2021-mimoqa}. More recently,
vision-language pre-training further boosts the power of
contextualized
representation~\cite{lu2019vilbert,ling-etal-2022-vision}. Recent
works on multimodality above raise challenges for future structured
prediction tasks and their independent factorization.

\subsection{Other Biases in Other Formalism}
\label{ssec:future:rep-bias-ways}

Besides the above structural inductive bias on compositionality and
hierarchical structure, in the future, I will continue the study on how
to represent other inductive biases in other different ways.

\Paragraph{Declaritive Constraints and other latent Models}In the
future, we also inject other structural interdependence/constraints
with declarative tools, such as integer linear programming,
probabilistic neural logic rules. Furthermore, following the line of
latent variable models, we plan to study more ways to relax structural
inductive biases as continuous and differentiable latent variables in
the end2end deep learning.

\Paragraph{Approximate Bias} With limited observations and
resources~(time, memory, energy), our human intelligence of
generalizing to new environments makes us efficiently learn when
interacting with the world and other human beings. This efficiency
largely depends on many \kw{inductive biases} and \kw{approximate
  biases} from human intelligence~\cite{Gershman2021WhatMU}, which are
potentially helpful for machine intelligence. We also plan to work
on approximate biases for reasoning. Especially for deep learning, we
plan to design approximate inference methods for deep structured
prediction.

\Paragraph{Causality} Current factorization and Markov random
field-based formalism only can capture the correlation between
different variables. In the future, we will extend the formalism to
bayesian networks and intervention-based models.

\subsection{Learning and Transfering the Inductive Biases}
\label{ssec:future:bias-learn-transfer}

\Paragraph{Learn the Inductive Biases via Self-supervision}
Inspired by self-supervised pretraining in ELMo and
BERT~\cite{devlin2019bert}, our VLDB'2022
paper~\cite{paul2021database} extend a contrastive-learning method to
learn the representation for tree-structured database query
plans. With a large amount of raw database query plans, we calculate
the graph similarity metric~{\em Smatch}~\cite{Cai:2013wn} to
represent the degree of overlap between a pair of plans.\footnote{The
  Smatch score~([0,1]) between two tree-structure plans can be
  computed by graph matching optimization algorithm, such as Integer
  Linear Programming~(ILP) or Hill-climbing methods.}  After we get
the {\em Smatch} scores $s_{ij}$ of each plan-pair $<p_{i}, p_{j}>$,
this can easily form a large dataset with {\em Smatch} score as the
contrastive self-supervision. In our experiments on the downstream
applications, we show that the structure encoder pre-trained from this
task can be easily finetuned for a new task or domain.

\Paragraph{Transfering Inductive Bias via Supplementary Training}
Learning to learn is an essential inductive bias in human
intelligence~\cite{harlow1949formation}, human can generalize
experience learned from similar tasks to learn new tasks. Nowadays
many datasets and pre-trained models are publicly accessible, besides
transferring the inductive bias from initial language models, I also
studied how to transfer the inductive biases learned from the
well-studied tasks to new tasks. My NAACL'2021 work on schema-guided
dialogue state tracking~\cite{cao2021schema} proposed to add a
supplementary pretraining phase on an intermediate task between the
pretraining-finetuning framework. Given a brand new task like
schema-guided dialogue state tracking, we show that supplementary
pretraining on intermediate tasks with similar problem structures will
offer efficient distributional inductive biases. More specifically, we
found that inductive bias learned in sentence-pair matching~(via
Natural Language Inference on SNLI) helps with intent classification
tasks, and span-based retrieval task structure~(via Question Answering
on SQuAD2) helps on the non-categorical slot labeling task.

Besides passively receiving fixed training data to learn, intelligent
systems can improve themselves by actively discovering more
supervisions. My future work will ground the explorations on two sub
areas:
\begin{inparaenum}[(a)]
\item learning on how to retrieve and integrate the experience with
  new observations. This is similar to how humans learn from the
  experience and other existing tools, including searching over
  similar datasets or pre-trained models.
\item learning on make the model self-evolve, such as via the
  feedback after deployment.
\end{inparaenum}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../dissertation-main.ltx"
%%% End:
