\BOOKMARK [0][-]{section*.1}{Abstract}{}% 1
\BOOKMARK [0][-]{section*.2}{LIST OF FIGURES}{}% 2
\BOOKMARK [0][-]{section*.3}{LIST OF TABLES}{}% 3
\BOOKMARK [0][-]{section*.4}{Notation and Symbols}{}% 4
\BOOKMARK [0][-]{chapter.1}{1. Introduction xxxx xxxxx xxx xx xxxx xxx xxxx xxxxxxxxxxxxxxxxxx xx xxxxxxxxxxxxxxxx xxxxxxxxxx xxxx}{}% 5
\BOOKMARK [1][-]{section.1.1}{1.1 Motivation}{chapter.1}% 6
\BOOKMARK [2][-]{subsection.1.1.1}{1.1.1 The Need for Inductive Bias}{section.1.1}% 7
\BOOKMARK [2][-]{subsection.1.1.2}{1.1.2 Where Do Model Get Inductive Biases}{section.1.1}% 8
\BOOKMARK [2][-]{subsection.1.1.3}{1.1.3 Thesis Statement}{section.1.1}% 9
\BOOKMARK [1][-]{section.1.2}{1.2 Contribuition and Roadmap}{chapter.1}% 10
\BOOKMARK [1][-]{section.1.3}{1.3 Roadmap}{chapter.1}% 11
\BOOKMARK [0][-]{chapter.2}{2. Structures in Natural Language}{}% 12
\BOOKMARK [1][-]{section.2.1}{2.1 Symbolic Representations for Natural Language}{chapter.2}% 13
\BOOKMARK [2][-]{subsection.2.1.1}{2.1.1 Broad-coverage Semantic Representation}{section.2.1}% 14
\BOOKMARK [3][-]{subsubsection.2.1.1.1}{2.1.1.1 Bi-lexical Semantic Dependency Parsing}{subsection.2.1.1}% 15
\BOOKMARK [3][-]{subsubsection.2.1.1.2}{2.1.1.2 Abstract Meaning Representation}{subsection.2.1.1}% 16
\BOOKMARK [3][-]{subsubsection.2.1.1.3}{2.1.1.3 Universal Conceptual Cognitive Anotation}{subsection.2.1.1}% 17
\BOOKMARK [2][-]{subsection.2.1.2}{2.1.2 Application-specific Representation: Dialog}{section.2.1}% 18
\BOOKMARK [3][-]{subsubsection.2.1.2.1}{2.1.2.1 Dialog Act}{subsection.2.1.2}% 19
\BOOKMARK [3][-]{subsubsection.2.1.2.2}{2.1.2.2 Dialog State Tracking}{subsection.2.1.2}% 20
\BOOKMARK [3][-]{subsubsection.2.1.2.3}{2.1.2.3 Conversational Semantic Representation}{subsection.2.1.2}% 21
\BOOKMARK [1][-]{section.2.2}{2.2 Chapter Summary}{chapter.2}% 22
\BOOKMARK [0][-]{chapter.3}{3. Unified Graph-based Parsing for Lexical-Anchoring}{}% 23
\BOOKMARK [1][-]{section.3.1}{3.1 Anchor Analysis on DM, PSD and AMR}{chapter.3}% 24
\BOOKMARK [2][-]{subsection.3.1.1}{3.1.1 Explicit Alignments: DM, PSD}{section.3.1}% 25
\BOOKMARK [2][-]{subsection.3.1.2}{3.1.2 Implicit Anchoring in AMR}{section.3.1}% 26
\BOOKMARK [3][-]{subsubsection.3.1.2.1}{3.1.2.1 AMR-to-String Alignments}{subsection.3.1.2}% 27
\BOOKMARK [3][-]{subsubsection.3.1.2.2}{3.1.2.2 AMR-to-Dependency Alignments}{subsection.3.1.2}% 28
\BOOKMARK [2][-]{subsection.3.1.3}{3.1.3 Lexical-Anchoring}{section.3.1}% 29
\BOOKMARK [1][-]{section.3.2}{3.2 Graph-based Parsing Framework with Latent Alignment}{chapter.3}% 30
\BOOKMARK [2][-]{subsection.3.2.1}{3.2.1 Alignment Model}{section.3.2}% 31
\BOOKMARK [3][-]{subsubsection.3.2.1.1}{3.2.1.1 Explicit Alignments}{subsection.3.2.1}% 32
\BOOKMARK [3][-]{subsubsection.3.2.1.2}{3.2.1.2 Implicit Alignments}{subsection.3.2.1}% 33
\BOOKMARK [2][-]{subsection.3.2.2}{3.2.2 Node Identification}{section.3.2}% 34
\BOOKMARK [2][-]{subsection.3.2.3}{3.2.3 Edge Identification}{section.3.2}% 35
\BOOKMARK [2][-]{subsection.3.2.4}{3.2.4 Inference}{section.3.2}% 36
\BOOKMARK [1][-]{section.3.3}{3.3 Latent Alignment Model}{chapter.3}% 37
\BOOKMARK [2][-]{subsection.3.3.1}{3.3.1 Continuous Relaxation for Discrete Alignments}{section.3.3}% 38
\BOOKMARK [2][-]{subsection.3.3.2}{3.3.2 VAE, Perturb-and-Map, Gumble Sinkhorn}{section.3.3}% 39
\BOOKMARK [1][-]{section.3.4}{3.4 Experiments and Results}{chapter.3}% 40
\BOOKMARK [2][-]{subsection.3.4.1}{3.4.1 Dataset and Evaluation}{section.3.4}% 41
\BOOKMARK [2][-]{subsection.3.4.2}{3.4.2 Summary of Implementation}{section.3.4}% 42
\BOOKMARK [3][-]{subsubsection.3.4.2.1}{3.4.2.1 Top}{subsection.3.4.2}% 43
\BOOKMARK [3][-]{subsubsection.3.4.2.2}{3.4.2.2 Node}{subsection.3.4.2}% 44
\BOOKMARK [3][-]{subsubsection.3.4.2.3}{3.4.2.3 Edge}{subsection.3.4.2}% 45
\BOOKMARK [3][-]{subsubsection.3.4.2.4}{3.4.2.4 Connectivity}{subsection.3.4.2}% 46
\BOOKMARK [2][-]{subsection.3.4.3}{3.4.3 Model Setup}{section.3.4}% 47
\BOOKMARK [2][-]{subsection.3.4.4}{3.4.4 Results}{section.3.4}% 48
\BOOKMARK [3][-]{paragraph.3.4.4.0.1}{3.4.4.0.1 Official Results on Lexical Anchoring}{subsection.3.4.4}% 49
\BOOKMARK [4][-]{paragraph.3.4.4.0.2}{3.4.4.0.2 Official Results on Phrasal Anchoring}{paragraph.3.4.4.0.1}% 50
\BOOKMARK [2][-]{subsection.3.4.5}{3.4.5 Error Breakdown}{section.3.4}% 51
\BOOKMARK [3][-]{subsubsection.3.4.5.1}{3.4.5.1 Error Analysis on Lexical-Anchoring}{subsection.3.4.5}% 52
\BOOKMARK [3][-]{subsubsection.3.4.5.2}{3.4.5.2 Error Analysis on Phrasal-Anchoring}{subsection.3.4.5}% 53
\BOOKMARK [1][-]{section.3.5}{3.5 Related Work}{chapter.3}% 54
\BOOKMARK [1][-]{section.3.6}{3.6 Chapter Summary}{chapter.3}% 55
\BOOKMARK [0][-]{chapter.4}{4. Tree-Factorization for Phrasal-Anchoring}{}% 56
\BOOKMARK [1][-]{section.4.1}{4.1 Analysis on Phrasal Anchoring in UCCA and TOP}{chapter.4}% 57
\BOOKMARK [1][-]{section.4.2}{4.2 Minimal Span-based CKY Parsing Framework}{chapter.4}% 58
\BOOKMARK [2][-]{subsection.4.2.1}{4.2.1 Graph-to-CT Transformation}{section.4.2}% 59
\BOOKMARK [2][-]{subsection.4.2.2}{4.2.2 CKY Parsing}{section.4.2}% 60
\BOOKMARK [3][-]{paragraph.4.2.2.0.1}{4.2.2.0.1 Tree Factorization}{subsection.4.2.2}% 61
\BOOKMARK [4][-]{paragraph.4.2.2.0.2}{4.2.2.0.2 CKY Parsing}{paragraph.4.2.2.0.1}% 62
\BOOKMARK [2][-]{subsection.4.2.3}{4.2.3 Span Encoding}{section.4.2}% 63
\BOOKMARK [1][-]{section.4.3}{4.3 Experiemnts}{chapter.4}% 64
\BOOKMARK [1][-]{section.4.4}{4.4 Results}{chapter.4}% 65
\BOOKMARK [2][-]{subsection.4.4.1}{4.4.1 Results on UCCA}{section.4.4}% 66
\BOOKMARK [3][-]{subsubsection.4.4.1.1}{4.4.1.1 Error Breakdown on UCCA}{subsection.4.4.1}% 67
\BOOKMARK [1][-]{section.4.5}{4.5 Related Work}{chapter.4}% 68
\BOOKMARK [1][-]{section.4.6}{4.6 Chapter Summary}{chapter.4}% 69
\BOOKMARK [0][-]{chapter.5}{5. Modeling on Sentence-Anchoring for Dialog in Therapy}{}% 70
\BOOKMARK [1][-]{section.5.1}{5.1 Background and Motivation}{chapter.5}% 71
\BOOKMARK [1][-]{section.5.2}{5.2 Task Definitions}{chapter.5}% 72
\BOOKMARK [1][-]{section.5.3}{5.3 Models for MISC Prediction}{chapter.5}% 73
\BOOKMARK [2][-]{subsection.5.3.1}{5.3.1 Encoding Dialogue}{section.5.3}% 74
\BOOKMARK [2][-]{subsection.5.3.2}{5.3.2 Word-level Attention}{section.5.3}% 75
\BOOKMARK [2][-]{subsection.5.3.3}{5.3.3 Utterance-level Attention}{section.5.3}% 76
\BOOKMARK [2][-]{subsection.5.3.4}{5.3.4 Predicting and Training}{section.5.3}% 77
\BOOKMARK [2][-]{subsection.5.3.5}{5.3.5 Addressing Label Imbalance}{section.5.3}% 78
\BOOKMARK [1][-]{section.5.4}{5.4 Experiments}{chapter.5}% 79
\BOOKMARK [2][-]{subsection.5.4.1}{5.4.1 Preprocessing and Model Setup}{section.5.4}% 80
\BOOKMARK [2][-]{subsection.5.4.2}{5.4.2 Results}{section.5.4}% 81
\BOOKMARK [2][-]{subsection.5.4.3}{5.4.3 Best Models}{section.5.4}% 82
\BOOKMARK [1][-]{section.5.5}{5.5 Analysis and Ablations}{chapter.5}% 83
\BOOKMARK [2][-]{subsection.5.5.1}{5.5.1 Label Confusion and Error Breakdown}{section.5.5}% 84
\BOOKMARK [2][-]{subsection.5.5.2}{5.5.2 How Context and Attention Help?}{section.5.5}% 85
\BOOKMARK [2][-]{subsection.5.5.3}{5.5.3 Can We Suggest Empathetic Responses?}{section.5.5}% 86
\BOOKMARK [1][-]{section.5.6}{5.6 Conclusion}{chapter.5}% 87
\BOOKMARK [0][-]{chapter.6}{6. Representing Intent/Slot Concept with Natural Language Description}{}% 88
\BOOKMARK [1][-]{section.6.1}{6.1 Introduction}{chapter.6}% 89
\BOOKMARK [1][-]{section.6.2}{6.2 Schema-Guided Dialog State Tracking}{chapter.6}% 90
\BOOKMARK [1][-]{section.6.3}{6.3 Datasets}{chapter.6}% 91
\BOOKMARK [1][-]{section.6.4}{6.4 Dialog \046 Schema Representation and Inference \(Q1\)}{chapter.6}% 92
\BOOKMARK [2][-]{subsection.6.4.1}{6.4.1 Encoder Architectures}{section.6.4}% 93
\BOOKMARK [2][-]{subsection.6.4.2}{6.4.2 Model Overview}{section.6.4}% 94
\BOOKMARK [2][-]{subsection.6.4.3}{6.4.3 Experiments on Encoder Comparison}{section.6.4}% 95
\BOOKMARK [1][-]{section.6.5}{6.5 Supplementary Training \(Q2\)}{chapter.6}% 96
\BOOKMARK [2][-]{subsection.6.5.1}{6.5.1 Intermediate Tasks}{section.6.5}% 97
\BOOKMARK [2][-]{subsection.6.5.2}{6.5.2 Results on Supplementary Training}{section.6.5}% 98
\BOOKMARK [1][-]{section.6.6}{6.6 Impact of Description Styles \(Q3\)}{chapter.6}% 99
\BOOKMARK [2][-]{subsection.6.6.1}{6.6.1 Benchmarking Styles}{section.6.6}% 100
\BOOKMARK [2][-]{subsection.6.6.2}{6.6.2 Results on Description Styles}{section.6.6}% 101
\BOOKMARK [3][-]{subsubsection.6.6.2.1}{6.6.2.1 Homogeneous Evaluation}{subsection.6.6.2}% 102
\BOOKMARK [3][-]{subsubsection.6.6.2.2}{6.6.2.2 Heterogeneous}{subsection.6.6.2}% 103
\BOOKMARK [1][-]{section.6.7}{6.7 Related Work}{chapter.6}% 104
\BOOKMARK [1][-]{section.6.8}{6.8 Conclusion}{chapter.6}% 105
\BOOKMARK [0][-]{chapter.7}{7. Conclusions and Future Work}{}% 106
\BOOKMARK [1][-]{section.7.1}{7.1 Claims and Research Contribution Revisited}{chapter.7}% 107
\BOOKMARK [1][-]{section.7.2}{7.2 Future Work Direction}{chapter.7}% 108
\BOOKMARK [1][-]{section.7.3}{7.3 Summary}{chapter.7}% 109
\BOOKMARK [0][-]{appendix.A}{A. The First}{}% 110
\BOOKMARK [0][-]{appendix.B}{B. The Second}{}% 111
\BOOKMARK [0][-]{appendix.C}{C. The Third}{}% 112
\BOOKMARK [0][-]{section*.40}{REFERENCES}{}% 113
\BOOKMARK [0][-]{section*.40}{Binomial Nomenclature Index}{}% 114
\BOOKMARK [0][-]{section*.40}{Topic Index}{}% 115
